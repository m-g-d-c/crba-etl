{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Required standard libraries\n",
    "import pandas as pd\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import bs4 as bs\n",
    "import selenium\n",
    "import html5lib\n",
    "#import nltk\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "\n",
    "# Extractors \n",
    "import extract\n",
    "\n",
    "# Cleansers (cluster specific)\n",
    "import cleanse\n",
    "\n",
    "# Normalizer (generalised across all clusters)\n",
    "from normalize import scaler\n",
    "\n",
    "# Utils\n",
    "from utils import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the export path for all data exports\n",
    "from pathlib import Path\n",
    "\n",
    "# Current working directory\n",
    "cwd = Path('.')\n",
    "\n",
    "# Folder with data-in artifacts, quired to run this script\n",
    "data_in = cwd / 'data_in'\n",
    "\n",
    "# Folder to export raw data\n",
    "data_sources_raw = cwd / 'data_out' / 'data_raw'\n",
    "data_sources_raw.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Folder to export cleansed data\n",
    "data_sources_cleansed = cwd / 'data_out' / 'data_cleansed'\n",
    "data_sources_cleansed.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Folder to export normalized data\n",
    "data_sources_normalized = cwd / 'data_out' / 'data_normalized'\n",
    "data_sources_normalized.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load country list and mapping dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of countries which contains all different variations of country names \n",
    "country_full_list = pd.read_excel(\n",
    "    data_in / 'all_countrynames_list.xlsx',\n",
    "    keep_default_na = False).drop_duplicates()\n",
    "\n",
    "# Create a version of the list with unique ISO2 and ISO3 codes\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'COUNTRY_ISO_2')\n",
    "\n",
    "# Country CRBA list, this is the list of the countries that should be in the final CRBA indicator list\n",
    "country_crba_list = pd.read_excel(\n",
    "    data_in / 'crba_country_list.xlsx',\n",
    "    header = None,\n",
    "    usecols = [0, 1], \n",
    "    names = ['COUNTRY_ISO_3', 'COUNTRY_NAME']).merge(\n",
    "        right = country_iso_list[['COUNTRY_ISO_2', 'COUNTRY_ISO_3']],\n",
    "        how = 'left',\n",
    "        on='COUNTRY_ISO_3',\n",
    "        validate = 'one_to_one')\n",
    "\n",
    "# Run the column mapper script to load the mapping dictionary\n",
    "with open(data_in / 'column_mapping.py') as file:\n",
    "    exec(file.read())\n",
    "\n",
    "# Run the column mapper script to load the mapping dictionary\n",
    "with open(data_in / 'value_mapping.py') as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources sheet\n",
    "crba_data_dictionary_source = pd.read_excel(\n",
    "    data_in / 'indicator_dictionary_CRBA.xlsx',\n",
    "    sheet_name = \"Source\",\n",
    "    keep_default_na = False\n",
    ")\n",
    "\n",
    "# snapshot sheet\n",
    "crba_data_dictionary_snapshot = pd.read_excel(\n",
    "    data_in / 'indicator_dictionary_CRBA.xlsx',\n",
    "    sheet_name = \"Snapshot\",\n",
    "    keep_default_na = False\n",
    ")\n",
    "\n",
    "# indicator sheet\n",
    "crba_data_dictionary_indicator = pd.read_excel(\n",
    "    data_in / 'indicator_dictionary_CRBA.xlsx',\n",
    "    sheet_name = \"Indicator\",\n",
    "    keep_default_na = False\n",
    ")\n",
    "\n",
    "# Input lists\n",
    "crba_data_dictionary_input_list = pd.read_excel(\n",
    "    data_in / 'indicator_dictionary_CRBA.xlsx',\n",
    "    sheet_name = \"Input_Lists\",\n",
    "    keep_default_na = False\n",
    ")\n",
    "\n",
    "# Add 2-digit shortcodes of index, issue and category to indicators sheet\n",
    "crba_data_dictionary_indicator = crba_data_dictionary_indicator.merge(\n",
    "    right=crba_data_dictionary_input_list[['INDEX', 'INDEX_CODE']],\n",
    "    left_on='INDEX',\n",
    "    right_on='INDEX'\n",
    ").merge(\n",
    "    right=crba_data_dictionary_input_list[['ISSUE', 'ISSUE_CODE']],\n",
    "    left_on='ISSUE',\n",
    "    right_on='ISSUE'\n",
    ").merge(\n",
    "    right=crba_data_dictionary_input_list[['CATEGORY', 'CATEGORY_CODE']],\n",
    "    left_on='CATEGORY',\n",
    "    right_on='CATEGORY'\n",
    ")\n",
    "\n",
    "# Create indicator code prefix (INDEX-ISSUE_CAEGORY CODE)\n",
    "crba_data_dictionary_indicator = crba_data_dictionary_indicator.assign(\n",
    "    INDICATOR_CODE_PREFIX = crba_data_dictionary_indicator.INDEX_CODE +\n",
    "    \"_\" +\n",
    "    crba_data_dictionary_indicator.ISSUE_CODE+\n",
    "    \"_\"+\n",
    "    crba_data_dictionary_indicator.CATEGORY_CODE+\n",
    "    \"_\")\n",
    "\n",
    "# Create indicator code\n",
    "crba_data_dictionary_indicator = crba_data_dictionary_indicator.assign(\n",
    "    INDICATOR_CODE = crba_data_dictionary_indicator.INDICATOR_CODE_PREFIX + crba_data_dictionary_indicator.INDICATOR_NAME.apply(\n",
    "    lambda x: utils.create_ind_code(x)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib, inspect\n",
    "\n",
    "extractors = { \n",
    "    cls.type: cls for name, cls in inspect.getmembers(\n",
    "        importlib.import_module(\"extract\"), \n",
    "        inspect.isclass\n",
    "    ) if hasattr(cls, 'type')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Staging (pre-processing) to create exceptional indicators´ raw data \n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process S-180 and S-181\n",
    "# Important: File requires having filepaths from above defined and pandas already imported\n",
    "\n",
    "with open(data_in / 'staging_create_raw_data.py') as file:\n",
    "    exec(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract - Transform - Load Loop\n",
    "## API sources\n",
    "### CSV API sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV sources\n",
    "api_sources = crba_data_dictionary_source[\n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (ILO)\") | \n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (UNESCO)\") | \n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (WHO)\") | \n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (UNICEF)\")\n",
    "].merge(\n",
    "    right = crba_data_dictionary_snapshot,\n",
    "    on = \"SOURCE_ID\"\n",
    ").merge(\n",
    "    right = crba_data_dictionary_indicator,\n",
    "    on = 'INDICATOR_ID'\n",
    ")\n",
    "\n",
    "# define emty dataframe\n",
    "combined_cleansed_csv = pd.DataFrame()\n",
    "combined_normalized_csv = pd.DataFrame()\n",
    "\n",
    "# Loop to extract data from API sources\n",
    "for index, row in api_sources.iterrows():\n",
    "    # Log\n",
    "    print(\"\\n - - - - - \\n Extracting source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Extraction section\n",
    "    try:\n",
    "        # Extract data\n",
    "        dataframe = extract.CSVExtractor.extract(url = row[\"ENDPOINT_URL\"])\n",
    "        \n",
    "        # Save raw data\n",
    "        dataframe.to_csv(\n",
    "            data_sources_raw / str(row[\"SOURCE_ID\"] + \"_raw.csv\"),\n",
    "            sep = \";\"\n",
    "            )\n",
    "    \n",
    "    except:\n",
    "       print(\"There was a problem with extraction of source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    \n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Cleansing\n",
    "    dataframe = cleanse.Cleanser().extract_who_raw_data(\n",
    "        raw_data=dataframe,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        display_value_col=\"Display Value\"\n",
    "    )\n",
    "    \n",
    "    dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "        raw_data=dataframe,\n",
    "        mapping_dictionary=mapping_dict,\n",
    "        final_sdmx_col_list=sdmx_df_columns_all\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().retrieve_latest_observation(\n",
    "        renamed_data=dataframe,\n",
    "        dim_cols = sdmx_df_columns_dims,\n",
    "        country_cols = sdmx_df_columns_country,\n",
    "        time_cols = sdmx_df_columns_time,\n",
    "        attr_cols=sdmx_df_columns_attr,\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "        grouped_data=dataframe,\n",
    "        crba_country_list=country_crba_list,\n",
    "        country_list_full = country_full_list\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_cols_fill_cells(\n",
    "        grouped_data_iso_filt=dataframe,\n",
    "        dim_cols=sdmx_df_columns_dims,\n",
    "        time_cols=sdmx_df_columns_time,\n",
    "        indicator_name_string=row[\"INDICATOR_NAME_x\"],\n",
    "        index_name_string=row[\"INDEX\"],\n",
    "        issue_name_string=row[\"ISSUE\"],\n",
    "        category_name_string=row[\"CATEGORY\"],\n",
    "        indicator_code_string=row[\"INDICATOR_CODE\"],\n",
    "        indicator_source_string=row[\"ADDRESS\"],\n",
    "        indicator_source_body_string=row[\"SOURCE_BODY\"],\n",
    "        indicator_description_string=row[\"INDICATOR_DESCRIPTION\"],\n",
    "        indicator_explanation_string=row[\"INDICATOR_EXPLANATION\"],\n",
    "        indicator_data_extraction_methodology_string=row[\"EXTRACTION_METHODOLOGY\"],\n",
    "        source_title_string=row[\"SOURCE_TITLE\"],\n",
    "        source_api_link_string=row[\"ENDPOINT_URL\"]\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().map_values(\n",
    "        cleansed_data = dataframe,\n",
    "        value_mapping_dict = value_mapper\n",
    "    )\n",
    "    \n",
    "    dataframe_cleansed = cleanse.Cleanser().encode_categorical_variables(\n",
    "        dataframe = dataframe,\n",
    "        encoding_string = row[\"VALUE_ENCODING\"],\n",
    "        encoding_labels = row[\"VALUE_LABELS\"]\n",
    "    )\n",
    "\n",
    "    cleanse.Cleanser().create_log_report(\n",
    "        cleansed_data=dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_cleansed_csv = combined_cleansed_csv.append(\n",
    "        other = dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Save cleansed data\n",
    "    dataframe_cleansed.to_csv(\n",
    "        data_sources_cleansed / str(row[\"SOURCE_ID\"] + \"_cleansed.csv\"),\n",
    "        sep = \";\")\n",
    "    \n",
    "    # Normalizing\n",
    "    dataframe_normalized = scaler.normalizer(\n",
    "        cleansed_data = dataframe_cleansed,\n",
    "        sql_subset_query_string=row[\"DIMENSION_VALUES_NORMALIZATION\"],\n",
    "        # dim_cols=sdmx_df_columns_dims,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        is_inverted = row[\"INVERT_NORMALIZATION\"],\n",
    "        whisker_factor=1.5,\n",
    "        raw_data_col=\"RAW_OBS_VALUE\",\n",
    "        scaled_data_col_name=\"SCALED_OBS_VALUE\",\n",
    "        maximum_score=10,\n",
    "        )\n",
    "    \n",
    "    dataframe_normalized.to_csv(\n",
    "        data_sources_normalized / str(row[\"SOURCE_ID\"] + \"_normalized.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_normalized_csv = combined_normalized_csv.append(\n",
    "        other = dataframe_normalized\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON API sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# JSON sources\n",
    "api_sources = crba_data_dictionary_source[\n",
    "    (crba_data_dictionary_source[\"SOURCE_TYPE\"] == \"API (SDG)\")\n",
    "].merge(\n",
    "    right = crba_data_dictionary_snapshot,\n",
    "    on = \"SOURCE_ID\"\n",
    ").merge(\n",
    "    right = crba_data_dictionary_indicator,\n",
    "    on = 'INDICATOR_ID'\n",
    ")\n",
    "\n",
    "# Loop to extract data from API sources\n",
    "for index, row in api_sources.iterrows():\n",
    "    # Log\n",
    "    print(\"\\n - - - - - \\n Extracting source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Exraction section\n",
    "    try:\n",
    "        # Extract data \n",
    "        dataframe = extract.JSONExtractor.extract(url = row[\"ENDPOINT_URL\"])\n",
    "        \n",
    "        # Save dataframe\n",
    "        dataframe.to_csv(\n",
    "            data_sources_raw / str(row[\"SOURCE_ID\"] + \"_raw.csv\"),\n",
    "            sep = \";\")\n",
    "    except:\n",
    "        print(\"There was an issue with source {}\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Cleansing in \n",
    "    dataframe = cleanse.Cleanser().extract_who_raw_data(\n",
    "        raw_data=dataframe,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        display_value_col=\"Display Value\"\n",
    "    )\n",
    "    \n",
    "    dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "        raw_data=dataframe,\n",
    "        mapping_dictionary=mapping_dict,\n",
    "        final_sdmx_col_list=sdmx_df_columns_all\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().retrieve_latest_observation(\n",
    "        renamed_data=dataframe,\n",
    "        dim_cols = sdmx_df_columns_dims,\n",
    "        country_cols = sdmx_df_columns_country,\n",
    "        time_cols = sdmx_df_columns_time,\n",
    "        attr_cols=sdmx_df_columns_attr,\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "        grouped_data=dataframe,\n",
    "        crba_country_list=country_crba_list,\n",
    "        country_list_full = country_full_list\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_cols_fill_cells(\n",
    "        grouped_data_iso_filt=dataframe,\n",
    "        dim_cols=sdmx_df_columns_dims,\n",
    "        time_cols=sdmx_df_columns_time,\n",
    "        indicator_name_string=row[\"INDICATOR_NAME_x\"],\n",
    "        index_name_string=row[\"INDEX\"],\n",
    "        issue_name_string=row[\"ISSUE\"],\n",
    "        category_name_string=row[\"CATEGORY\"],\n",
    "        indicator_code_string=row[\"INDICATOR_CODE\"],\n",
    "        indicator_source_string=row[\"ADDRESS\"],\n",
    "        indicator_source_body_string=row[\"SOURCE_BODY\"],\n",
    "        indicator_description_string=row[\"INDICATOR_DESCRIPTION\"],\n",
    "        source_title_string=row[\"SOURCE_TITLE\"],\n",
    "        indicator_explanation_string=row[\"INDICATOR_EXPLANATION\"],\n",
    "        indicator_data_extraction_methodology_string=row[\"EXTRACTION_METHODOLOGY\"],\n",
    "        source_api_link_string=row[\"ENDPOINT_URL\"]\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().map_values(\n",
    "        cleansed_data = dataframe,\n",
    "        value_mapping_dict = value_mapper\n",
    "    )\n",
    "    \n",
    "    dataframe_cleansed = cleanse.Cleanser().encode_categorical_variables(\n",
    "        dataframe = dataframe,\n",
    "        encoding_string = row[\"VALUE_ENCODING\"],\n",
    "        encoding_labels = row[\"VALUE_LABELS\"]\n",
    "    )\n",
    "\n",
    "    cleanse.Cleanser().create_log_report(\n",
    "        cleansed_data=dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_cleansed_csv = combined_cleansed_csv.append(\n",
    "        other = dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Save cleansed data\n",
    "    dataframe_cleansed.to_csv(\n",
    "        data_sources_cleansed / str(row[\"SOURCE_ID\"] + \"_cleansed.csv\"),\n",
    "        sep = \";\")\n",
    "    \n",
    "    # Normalizing section\n",
    "    dataframe_normalized = scaler.normalizer(\n",
    "        cleansed_data = dataframe_cleansed,\n",
    "        sql_subset_query_string=row[\"DIMENSION_VALUES_NORMALIZATION\"],\n",
    "        # dim_cols=sdmx_df_columns_dims,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        is_inverted = row[\"INVERT_NORMALIZATION\"],\n",
    "        whisker_factor=1.5,\n",
    "        raw_data_col=\"RAW_OBS_VALUE\",\n",
    "        scaled_data_col_name=\"SCALED_OBS_VALUE\",\n",
    "        maximum_score=10,\n",
    "        )\n",
    "    \n",
    "    dataframe_normalized.to_csv(\n",
    "        data_sources_normalized / str(row[\"SOURCE_ID\"] + \"_normalized.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_normalized_csv = combined_normalized_csv.append(\n",
    "        other = dataframe_normalized\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML Sources\n",
    "### UN Treaty Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UN Treaty HTML sources\n",
    "api_sources = crba_data_dictionary_source[\n",
    "    (crba_data_dictionary_source[\"SOURCE_BODY\"] == \"UN Treaties\")\n",
    "].merge(\n",
    "    right = crba_data_dictionary_snapshot,\n",
    "    on = \"SOURCE_ID\"\n",
    ").merge(\n",
    "    right = crba_data_dictionary_indicator,\n",
    "    on = 'INDICATOR_ID'\n",
    ")\n",
    "\n",
    "# Loop to extract data from API sources\n",
    "for index, row in api_sources.iterrows():\n",
    "    # Log\n",
    "    print(\"\\n - - - - - \\n Extracting source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Exraction section\n",
    "    dataframe = extract.HTMLExtractor().extract(url = row[\"ADDRESS\"])\n",
    "    \n",
    "    # Save dataframe\n",
    "    dataframe.to_csv(\n",
    "        data_sources_raw / str(row[\"SOURCE_ID\"] + \"_raw.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Cleansing\n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Cleansing\n",
    "    dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "        raw_data=dataframe,\n",
    "        mapping_dictionary=mapping_dict,\n",
    "        final_sdmx_col_list=sdmx_df_columns_all\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "        grouped_data=dataframe,\n",
    "        crba_country_list=country_crba_list,\n",
    "        country_list_full = country_full_list\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_cols_fill_cells(\n",
    "        grouped_data_iso_filt=dataframe,\n",
    "        dim_cols=sdmx_df_columns_dims,\n",
    "        time_cols=sdmx_df_columns_time,\n",
    "        indicator_name_string=row[\"INDICATOR_NAME_x\"],\n",
    "        index_name_string=row[\"INDEX\"],\n",
    "        issue_name_string=row[\"ISSUE\"],\n",
    "        category_name_string=row[\"CATEGORY\"],\n",
    "        indicator_code_string=row[\"INDICATOR_CODE\"],\n",
    "        indicator_source_string=row[\"ADDRESS\"],\n",
    "        indicator_source_body_string=row[\"SOURCE_BODY\"],\n",
    "        indicator_description_string=row[\"INDICATOR_DESCRIPTION\"],\n",
    "        source_title_string=row[\"SOURCE_TITLE\"],\n",
    "        indicator_explanation_string=row[\"INDICATOR_EXPLANATION\"],\n",
    "        indicator_data_extraction_methodology_string=row[\"EXTRACTION_METHODOLOGY\"],\n",
    "        source_api_link_string=row[\"ENDPOINT_URL\"]\n",
    "    )\n",
    "\n",
    "    dataframe_cleansed = cleanse.Cleanser().encode_ilo_un_treaty_data(\n",
    "        dataframe = dataframe,\n",
    "        treaty_source_body = row[\"SOURCE_BODY\"]\n",
    "    )\n",
    "\n",
    "    cleanse.Cleanser().create_log_report(\n",
    "        cleansed_data=dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_cleansed_csv = combined_cleansed_csv.append(\n",
    "        other = dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    # Save cleansed data\n",
    "    dataframe_cleansed.to_csv(\n",
    "        data_sources_cleansed / str(row[\"SOURCE_ID\"] + \"_cleansed.csv\"),\n",
    "        sep = \";\")\n",
    "    \n",
    "    # Normalizing section\n",
    "    dataframe_normalized = scaler.normalizer(\n",
    "        cleansed_data = dataframe_cleansed,\n",
    "        sql_subset_query_string=row[\"DIMENSION_VALUES_NORMALIZATION\"],\n",
    "        # dim_cols=sdmx_df_columns_dims,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        is_inverted = row[\"INVERT_NORMALIZATION\"],\n",
    "        whisker_factor=1.5,\n",
    "        raw_data_col=\"RAW_OBS_VALUE\",\n",
    "        scaled_data_col_name=\"SCALED_OBS_VALUE\",\n",
    "        maximum_score=10,\n",
    "        )\n",
    "    \n",
    "    dataframe_normalized.to_csv(\n",
    "        data_sources_normalized / str(row[\"SOURCE_ID\"] + \"_normalized.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_normalized_csv = combined_normalized_csv.append(\n",
    "        other = dataframe_normalized\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export concatented dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # # CLEANSED DATA\n",
    "\n",
    "# Idenify all dimension columns in combined dataframe\n",
    "available_dim_cols = []\n",
    "for col in combined_cleansed_csv.columns:\n",
    "    dim_col = re.findall(\"DIM_.+\", col)\n",
    "    # print(dim_col)\n",
    "    if len(dim_col) == 1:\n",
    "        available_dim_cols += dim_col\n",
    "\n",
    "# Fill _T for all NA values of dimension columns\n",
    "# 5b Fill in current year for time variable\n",
    "combined_cleansed_csv[available_dim_cols] = combined_cleansed_csv[\n",
    "    available_dim_cols\n",
    "].fillna(value=\"_T\")\n",
    "\n",
    "# Export combined cleansed dataframe as a sample\n",
    "combined_cleansed_csv.to_csv(\n",
    "    path_or_buf = cwd / 'data_out' / 'combined_cleansed.csv',\n",
    "    sep = \";\"\n",
    ")\n",
    "\n",
    "# # # # # NORMALIZED DATA\n",
    "\n",
    "# Idenify all dimension columns in combined dataframe\n",
    "available_dim_cols = []\n",
    "for col in combined_normalized_csv.columns:\n",
    "    dim_col = re.findall(\"DIM_.+\", col)\n",
    "    # print(dim_col)\n",
    "    if len(dim_col) == 1:\n",
    "        available_dim_cols += dim_col\n",
    "\n",
    "# Fill _T for all NA values of dimension columns\n",
    "# 5b Fill in current year for time variable\n",
    "combined_normalized_csv[available_dim_cols] = combined_normalized_csv[\n",
    "    available_dim_cols\n",
    "].fillna(value=\"_T\")\n",
    "\n",
    "# Export combined cleansed dataframe as a sample\n",
    "combined_normalized_csv.to_csv(\n",
    "    path_or_buf = cwd / 'data_out' / 'combined_normalized.csv',\n",
    "    sep = \";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEVELOPMENT AND TRASH AREA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract selenium sources --> This code is stable as of 06.11.20, TO DO is to put this into a loop (which must be done in container, so I can only do it once James has looked at the issue with Chrome driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Michael\\anaconda3\\envs\\unicef-test\\lib\\site-packages\\ipykernel_launcher.py:32: DeprecationWarning: use options instead of chrome_options\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Calling function 'rename_and_discard_columns'...\n",
      "\n",
      " Calling function 'add_and_discard_countries'...\n",
      "\n",
      " Calling function 'add_cols_fill_cells'...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COUNTRY_NAME</th>\n",
       "      <th>ATTR_TREATY_STATUS</th>\n",
       "      <th>ATTR_FOOTNOTE_OF_SOURCE</th>\n",
       "      <th>COUNTRY_ISO_2</th>\n",
       "      <th>COUNTRY_ISO_3</th>\n",
       "      <th>_merge</th>\n",
       "      <th>RAW_OBS_VALUE</th>\n",
       "      <th>ATTR_ENCODING_LABELS</th>\n",
       "      <th>SCALED_OBS_VALUE</th>\n",
       "      <th>OBS_STATUS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>In Force</td>\n",
       "      <td></td>\n",
       "      <td>AF</td>\n",
       "      <td>AFG</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Albania</td>\n",
       "      <td>In Force</td>\n",
       "      <td>Excluding Article 11 by virtue of the ratifica...</td>\n",
       "      <td>AL</td>\n",
       "      <td>ALB</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Algeria</td>\n",
       "      <td>In Force</td>\n",
       "      <td></td>\n",
       "      <td>DZ</td>\n",
       "      <td>DZA</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>In Force</td>\n",
       "      <td></td>\n",
       "      <td>AR</td>\n",
       "      <td>ARG</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Armenia</td>\n",
       "      <td>In Force</td>\n",
       "      <td>Excluding Article 11 by virtue of the ratifica...</td>\n",
       "      <td>AM</td>\n",
       "      <td>ARM</td>\n",
       "      <td>both</td>\n",
       "      <td>2</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>USA</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UZB</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VUT</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VNM</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>right_only</td>\n",
       "      <td>1</td>\n",
       "      <td>2=Yes, 1=No; as answer to the following questi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>197 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    COUNTRY_NAME ATTR_TREATY_STATUS  \\\n",
       "0    Afghanistan           In Force   \n",
       "1        Albania           In Force   \n",
       "2        Algeria           In Force   \n",
       "3      Argentina           In Force   \n",
       "4        Armenia           In Force   \n",
       "..           ...                ...   \n",
       "192          NaN                NaN   \n",
       "193          NaN                NaN   \n",
       "194          NaN                NaN   \n",
       "195          NaN                NaN   \n",
       "196          NaN                NaN   \n",
       "\n",
       "                               ATTR_FOOTNOTE_OF_SOURCE COUNTRY_ISO_2  \\\n",
       "0                                                                 AF   \n",
       "1    Excluding Article 11 by virtue of the ratifica...            AL   \n",
       "2                                                                 DZ   \n",
       "3                                                                 AR   \n",
       "4    Excluding Article 11 by virtue of the ratifica...            AM   \n",
       "..                                                 ...           ...   \n",
       "192                                                NaN           NaN   \n",
       "193                                                NaN           NaN   \n",
       "194                                                NaN           NaN   \n",
       "195                                                NaN           NaN   \n",
       "196                                                NaN           NaN   \n",
       "\n",
       "    COUNTRY_ISO_3      _merge RAW_OBS_VALUE  \\\n",
       "0             AFG        both             2   \n",
       "1             ALB        both             2   \n",
       "2             DZA        both             2   \n",
       "3             ARG        both             2   \n",
       "4             ARM        both             2   \n",
       "..            ...         ...           ...   \n",
       "192           USA  right_only             1   \n",
       "193           UZB  right_only             1   \n",
       "194           VUT  right_only             1   \n",
       "195           VNM  right_only             1   \n",
       "196           ZWE  right_only             1   \n",
       "\n",
       "                                  ATTR_ENCODING_LABELS  SCALED_OBS_VALUE  \\\n",
       "0    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "1    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "2    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "3    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "4    2=Yes, 1=No; as answer to the following questi...              10.0   \n",
       "..                                                 ...               ...   \n",
       "192  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "193  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "194  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "195  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "196  2=Yes, 1=No; as answer to the following questi...               0.0   \n",
       "\n",
       "    OBS_STATUS  \n",
       "0          nan  \n",
       "1          nan  \n",
       "2          nan  \n",
       "3          nan  \n",
       "4          nan  \n",
       "..         ...  \n",
       "192        nan  \n",
       "193        nan  \n",
       "194        nan  \n",
       "195        nan  \n",
       "196        nan  \n",
       "\n",
       "[197 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Specify location of chromedriver\n",
    "cwd = os.getcwd()\n",
    "driver_location = cwd + '\\\\chromedriver.exe'\n",
    "\n",
    "# Add option to make it headless (so that it doesn't open an actual chrome window)\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(driver_location, chrome_options=options)\n",
    "\n",
    "# Get HTTP response\n",
    "# response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312256:NO\")\n",
    "# response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO::P11300_INSTRUMENT_ID:312283\")\n",
    "# response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312328:NO\")\n",
    "response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312240:NO\")\n",
    "\n",
    "# Get response\n",
    "# response = driver.get(html_url)\n",
    "\n",
    "# Retrieve the actual html\n",
    "html = driver.page_source\n",
    "\n",
    "# Soupify\n",
    "soup = bs.BeautifulSoup(html)\n",
    "\n",
    "# Extract the target table as attribute\n",
    "target_table = str(\n",
    "    soup.find_all(\"table\", {\"cellspacing\": \"0\", \"class\": \"horizontalLine\"})\n",
    ")\n",
    "\n",
    "# Create dataframe with the data\n",
    "raw_data = pd.read_html(io=target_table, header=0)[\n",
    "    0\n",
    "]  # return is a list of DFs, specify [0] to get actual DF\n",
    "\n",
    "# Cleansing\n",
    "dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "    raw_data=raw_data,\n",
    "    mapping_dictionary=mapping_dict,\n",
    "    final_sdmx_col_list=sdmx_df_columns_all\n",
    ")\n",
    "\n",
    "dataframe = cleanse.Cleanser().decompose_country_footnote_ilo_normlex(\n",
    "    dataframe = dataframe,\n",
    "    country_name_list = country_full_list.COUNTRY_NAME\n",
    ")\n",
    "\n",
    "dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "    grouped_data=dataframe,\n",
    "    crba_country_list=country_crba_list,\n",
    "    country_list_full = country_full_list\n",
    ")\n",
    "\n",
    "dataframe_cleansed = cleanse.Cleanser().encode_ilo_un_treaty_data(\n",
    "    dataframe = dataframe,\n",
    "    treaty_source_body='ILO NORMLEX'\n",
    ")\n",
    "\n",
    "# Normalizing section\n",
    "dataframe_normalized = scaler.normalizer(\n",
    "    cleansed_data = dataframe_cleansed,\n",
    "    sql_subset_query_string=None\n",
    ")\n",
    "\n",
    "dataframe_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.sub(dataframe[\"ATTR_FOOTNOTE_OF_SOURCE\"][5]\n",
    "       \n",
    "# Speifically for ILO NORMLEX - extract country name if additonal info is given\n",
    "#dataframe[\"ATTR_FOOTNOTE_OF_SOURCE\"] = dataframe[\"COUNTRY_NAME\"]\n",
    "#dataframe[\"COUNTRY_NAME\"] = dataframe[\"COUNTRY_NAME\"].apply(extract_country_name)\n",
    "#dataframe[\"ATTR_FOOTNOTE_OF_SOURCE\"] = dataframe.apply(lambda x: re.sub(x['COUNTRY_NAME'], \"\", x[\"ATTR_FOOTNOTE_OF_SOURCE\"]), 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "import selenium\n",
    "import os\n",
    "from pathlib import Path\n",
    "from selenium import webdriver\n",
    "\n",
    "# cwd = Path('.')\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Current working directory\n",
    "driver_location = cwd + '\\\\geckodriver.exe'\n",
    "\n",
    "print(driver_location)\n",
    "\n",
    "# Open the targete html. Must be done with selenium, because it doesnt work with normal URL request\n",
    "#driver = webdriver.Firefox(executable_path=\"D:/Documents/2020/28_UNICEF/10_working_repo/data-etl/geckodriver.exe\")\n",
    "driver = webdriver.Firefox(executable_path=driver_location)\n",
    "\n",
    "# Get HTTP response\n",
    "response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312256:NO\")\n"
   ]
  },
  {
   "source": [
    "## World policy analsis centre loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    COUNTRY_ISO_3                       RAW_OBS_VALUE  TIME_PERIOD _merge  \\\n",
       "0             AFG  VALUE WITHOUT MAPPING - PLEASE MAP       2014.0   both   \n",
       "1             ALB  VALUE WITHOUT MAPPING - PLEASE MAP       2014.0   both   \n",
       "2             AND                                   0       2014.0   both   \n",
       "3             DZA  VALUE WITHOUT MAPPING - PLEASE MAP       2014.0   both   \n",
       "4             AGO  VALUE WITHOUT MAPPING - PLEASE MAP       2014.0   both   \n",
       "..            ...                                 ...          ...    ...   \n",
       "190           VEN  VALUE WITHOUT MAPPING - PLEASE MAP       2014.0   both   \n",
       "191           VNM  VALUE WITHOUT MAPPING - PLEASE MAP       2014.0   both   \n",
       "192           YEM  VALUE WITHOUT MAPPING - PLEASE MAP       2014.0   both   \n",
       "193           ZMB  VALUE WITHOUT MAPPING - PLEASE MAP       2014.0   both   \n",
       "194           ZWE                                   0       2014.0   both   \n",
       "\n",
       "     INDICATOR_NAME INDICATOR_INDEX            INDICATOR_ISSUE  \\\n",
       "0    Minimum wages.       Workplace  Decent working conditions   \n",
       "1    Minimum wages.       Workplace  Decent working conditions   \n",
       "2    Minimum wages.       Workplace  Decent working conditions   \n",
       "3    Minimum wages.       Workplace  Decent working conditions   \n",
       "4    Minimum wages.       Workplace  Decent working conditions   \n",
       "..              ...             ...                        ...   \n",
       "190  Minimum wages.       Workplace  Decent working conditions   \n",
       "191  Minimum wages.       Workplace  Decent working conditions   \n",
       "192  Minimum wages.       Workplace  Decent working conditions   \n",
       "193  Minimum wages.       Workplace  Decent working conditions   \n",
       "194  Minimum wages.       Workplace  Decent working conditions   \n",
       "\n",
       "    INDICATOR_CATEGORY   INDICATOR_CODE  \\\n",
       "0              Outcome  WP_DW_OC_MINWAG   \n",
       "1              Outcome  WP_DW_OC_MINWAG   \n",
       "2              Outcome  WP_DW_OC_MINWAG   \n",
       "3              Outcome  WP_DW_OC_MINWAG   \n",
       "4              Outcome  WP_DW_OC_MINWAG   \n",
       "..                 ...              ...   \n",
       "190            Outcome  WP_DW_OC_MINWAG   \n",
       "191            Outcome  WP_DW_OC_MINWAG   \n",
       "192            Outcome  WP_DW_OC_MINWAG   \n",
       "193            Outcome  WP_DW_OC_MINWAG   \n",
       "194            Outcome  WP_DW_OC_MINWAG   \n",
       "\n",
       "                                           ATTR_SOURCE  \\\n",
       "0    https://www.worldpolicycenter.org/policies/at-...   \n",
       "1    https://www.worldpolicycenter.org/policies/at-...   \n",
       "2    https://www.worldpolicycenter.org/policies/at-...   \n",
       "3    https://www.worldpolicycenter.org/policies/at-...   \n",
       "4    https://www.worldpolicycenter.org/policies/at-...   \n",
       "..                                                 ...   \n",
       "190  https://www.worldpolicycenter.org/policies/at-...   \n",
       "191  https://www.worldpolicycenter.org/policies/at-...   \n",
       "192  https://www.worldpolicycenter.org/policies/at-...   \n",
       "193  https://www.worldpolicycenter.org/policies/at-...   \n",
       "194  https://www.worldpolicycenter.org/policies/at-...   \n",
       "\n",
       "                 ATTR_SOURCE_BODY  \\\n",
       "0    World Policy Analysis Centre   \n",
       "1    World Policy Analysis Centre   \n",
       "2    World Policy Analysis Centre   \n",
       "3    World Policy Analysis Centre   \n",
       "4    World Policy Analysis Centre   \n",
       "..                            ...   \n",
       "190  World Policy Analysis Centre   \n",
       "191  World Policy Analysis Centre   \n",
       "192  World Policy Analysis Centre   \n",
       "193  World Policy Analysis Centre   \n",
       "194  World Policy Analysis Centre   \n",
       "\n",
       "                       ATTR_INDICATOR_DESCRIPTION ATTR_INDICATOR_EXPLANATION  \\\n",
       "0    At what level are minimum wages set per day?                              \n",
       "1    At what level are minimum wages set per day?                              \n",
       "2    At what level are minimum wages set per day?                              \n",
       "3    At what level are minimum wages set per day?                              \n",
       "4    At what level are minimum wages set per day?                              \n",
       "..                                            ...                        ...   \n",
       "190  At what level are minimum wages set per day?                              \n",
       "191  At what level are minimum wages set per day?                              \n",
       "192  At what level are minimum wages set per day?                              \n",
       "193  At what level are minimum wages set per day?                              \n",
       "194  At what level are minimum wages set per day?                              \n",
       "\n",
       "    ATTR_DATA_EXTRACTION_METHDOLOGY  \\\n",
       "0                                     \n",
       "1                                     \n",
       "2                                     \n",
       "3                                     \n",
       "4                                     \n",
       "..                              ...   \n",
       "190                                   \n",
       "191                                   \n",
       "192                                   \n",
       "193                                   \n",
       "194                                   \n",
       "\n",
       "                                 ATTR_SOURCE_TITLE ATTR_API_ENDPOINT_URL  \\\n",
       "0    At what level are minimum wages set per day?                          \n",
       "1    At what level are minimum wages set per day?                          \n",
       "2    At what level are minimum wages set per day?                          \n",
       "3    At what level are minimum wages set per day?                          \n",
       "4    At what level are minimum wages set per day?                          \n",
       "..                                             ...                   ...   \n",
       "190  At what level are minimum wages set per day?                          \n",
       "191  At what level are minimum wages set per day?                          \n",
       "192  At what level are minimum wages set per day?                          \n",
       "193  At what level are minimum wages set per day?                          \n",
       "194  At what level are minimum wages set per day?                          \n",
       "\n",
       "     CRBA_RELEASE_YEAR                               ATTR_ENCODING_LABELS  \n",
       "0                 2020  6 = Collective bargaining; 5 = Over $10.00 PPP...  \n",
       "1                 2020  6 = Collective bargaining; 5 = Over $10.00 PPP...  \n",
       "2                 2020  6 = Collective bargaining; 5 = Over $10.00 PPP...  \n",
       "3                 2020  6 = Collective bargaining; 5 = Over $10.00 PPP...  \n",
       "4                 2020  6 = Collective bargaining; 5 = Over $10.00 PPP...  \n",
       "..                 ...                                                ...  \n",
       "190               2020  6 = Collective bargaining; 5 = Over $10.00 PPP...  \n",
       "191               2020  6 = Collective bargaining; 5 = Over $10.00 PPP...  \n",
       "192               2020  6 = Collective bargaining; 5 = Over $10.00 PPP...  \n",
       "193               2020  6 = Collective bargaining; 5 = Over $10.00 PPP...  \n",
       "194               2020  6 = Collective bargaining; 5 = Over $10.00 PPP...  \n",
       "\n",
       "[195 rows x 18 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>COUNTRY_ISO_3</th>\n      <th>RAW_OBS_VALUE</th>\n      <th>TIME_PERIOD</th>\n      <th>_merge</th>\n      <th>INDICATOR_NAME</th>\n      <th>INDICATOR_INDEX</th>\n      <th>INDICATOR_ISSUE</th>\n      <th>INDICATOR_CATEGORY</th>\n      <th>INDICATOR_CODE</th>\n      <th>ATTR_SOURCE</th>\n      <th>ATTR_SOURCE_BODY</th>\n      <th>ATTR_INDICATOR_DESCRIPTION</th>\n      <th>ATTR_INDICATOR_EXPLANATION</th>\n      <th>ATTR_DATA_EXTRACTION_METHDOLOGY</th>\n      <th>ATTR_SOURCE_TITLE</th>\n      <th>ATTR_API_ENDPOINT_URL</th>\n      <th>CRBA_RELEASE_YEAR</th>\n      <th>ATTR_ENCODING_LABELS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AFG</td>\n      <td>VALUE WITHOUT MAPPING - PLEASE MAP</td>\n      <td>2014.0</td>\n      <td>both</td>\n      <td>Minimum wages.</td>\n      <td>Workplace</td>\n      <td>Decent working conditions</td>\n      <td>Outcome</td>\n      <td>WP_DW_OC_MINWAG</td>\n      <td>https://www.worldpolicycenter.org/policies/at-...</td>\n      <td>World Policy Analysis Centre</td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td></td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td>2020</td>\n      <td>6 = Collective bargaining; 5 = Over $10.00 PPP...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ALB</td>\n      <td>VALUE WITHOUT MAPPING - PLEASE MAP</td>\n      <td>2014.0</td>\n      <td>both</td>\n      <td>Minimum wages.</td>\n      <td>Workplace</td>\n      <td>Decent working conditions</td>\n      <td>Outcome</td>\n      <td>WP_DW_OC_MINWAG</td>\n      <td>https://www.worldpolicycenter.org/policies/at-...</td>\n      <td>World Policy Analysis Centre</td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td></td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td>2020</td>\n      <td>6 = Collective bargaining; 5 = Over $10.00 PPP...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AND</td>\n      <td>0</td>\n      <td>2014.0</td>\n      <td>both</td>\n      <td>Minimum wages.</td>\n      <td>Workplace</td>\n      <td>Decent working conditions</td>\n      <td>Outcome</td>\n      <td>WP_DW_OC_MINWAG</td>\n      <td>https://www.worldpolicycenter.org/policies/at-...</td>\n      <td>World Policy Analysis Centre</td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td></td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td>2020</td>\n      <td>6 = Collective bargaining; 5 = Over $10.00 PPP...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>DZA</td>\n      <td>VALUE WITHOUT MAPPING - PLEASE MAP</td>\n      <td>2014.0</td>\n      <td>both</td>\n      <td>Minimum wages.</td>\n      <td>Workplace</td>\n      <td>Decent working conditions</td>\n      <td>Outcome</td>\n      <td>WP_DW_OC_MINWAG</td>\n      <td>https://www.worldpolicycenter.org/policies/at-...</td>\n      <td>World Policy Analysis Centre</td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td></td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td>2020</td>\n      <td>6 = Collective bargaining; 5 = Over $10.00 PPP...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AGO</td>\n      <td>VALUE WITHOUT MAPPING - PLEASE MAP</td>\n      <td>2014.0</td>\n      <td>both</td>\n      <td>Minimum wages.</td>\n      <td>Workplace</td>\n      <td>Decent working conditions</td>\n      <td>Outcome</td>\n      <td>WP_DW_OC_MINWAG</td>\n      <td>https://www.worldpolicycenter.org/policies/at-...</td>\n      <td>World Policy Analysis Centre</td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td></td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td>2020</td>\n      <td>6 = Collective bargaining; 5 = Over $10.00 PPP...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>VEN</td>\n      <td>VALUE WITHOUT MAPPING - PLEASE MAP</td>\n      <td>2014.0</td>\n      <td>both</td>\n      <td>Minimum wages.</td>\n      <td>Workplace</td>\n      <td>Decent working conditions</td>\n      <td>Outcome</td>\n      <td>WP_DW_OC_MINWAG</td>\n      <td>https://www.worldpolicycenter.org/policies/at-...</td>\n      <td>World Policy Analysis Centre</td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td></td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td>2020</td>\n      <td>6 = Collective bargaining; 5 = Over $10.00 PPP...</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>VNM</td>\n      <td>VALUE WITHOUT MAPPING - PLEASE MAP</td>\n      <td>2014.0</td>\n      <td>both</td>\n      <td>Minimum wages.</td>\n      <td>Workplace</td>\n      <td>Decent working conditions</td>\n      <td>Outcome</td>\n      <td>WP_DW_OC_MINWAG</td>\n      <td>https://www.worldpolicycenter.org/policies/at-...</td>\n      <td>World Policy Analysis Centre</td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td></td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td>2020</td>\n      <td>6 = Collective bargaining; 5 = Over $10.00 PPP...</td>\n    </tr>\n    <tr>\n      <th>192</th>\n      <td>YEM</td>\n      <td>VALUE WITHOUT MAPPING - PLEASE MAP</td>\n      <td>2014.0</td>\n      <td>both</td>\n      <td>Minimum wages.</td>\n      <td>Workplace</td>\n      <td>Decent working conditions</td>\n      <td>Outcome</td>\n      <td>WP_DW_OC_MINWAG</td>\n      <td>https://www.worldpolicycenter.org/policies/at-...</td>\n      <td>World Policy Analysis Centre</td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td></td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td>2020</td>\n      <td>6 = Collective bargaining; 5 = Over $10.00 PPP...</td>\n    </tr>\n    <tr>\n      <th>193</th>\n      <td>ZMB</td>\n      <td>VALUE WITHOUT MAPPING - PLEASE MAP</td>\n      <td>2014.0</td>\n      <td>both</td>\n      <td>Minimum wages.</td>\n      <td>Workplace</td>\n      <td>Decent working conditions</td>\n      <td>Outcome</td>\n      <td>WP_DW_OC_MINWAG</td>\n      <td>https://www.worldpolicycenter.org/policies/at-...</td>\n      <td>World Policy Analysis Centre</td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td></td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td>2020</td>\n      <td>6 = Collective bargaining; 5 = Over $10.00 PPP...</td>\n    </tr>\n    <tr>\n      <th>194</th>\n      <td>ZWE</td>\n      <td>0</td>\n      <td>2014.0</td>\n      <td>both</td>\n      <td>Minimum wages.</td>\n      <td>Workplace</td>\n      <td>Decent working conditions</td>\n      <td>Outcome</td>\n      <td>WP_DW_OC_MINWAG</td>\n      <td>https://www.worldpolicycenter.org/policies/at-...</td>\n      <td>World Policy Analysis Centre</td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td></td>\n      <td>At what level are minimum wages set per day?</td>\n      <td></td>\n      <td>2020</td>\n      <td>6 = Collective bargaining; 5 = Over $10.00 PPP...</td>\n    </tr>\n  </tbody>\n</table>\n<p>195 rows × 18 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n - - - - - \n Extracting source S-49 \n\n\n Extracting data and store it as raw data\nThere was an issue with source S-49\n\n - - - - - \n Cleansing source S-49 \n\n\n - - - - - \n Cleansing source S-49 \n\n\n Calling function 'rename_and_discard_columns'...\n[  4.   5.  nan   3.   1.   2. 999.]\n\n Calling function 'add_and_discard_countries'...\n\n Calling function 'add_cols_fill_cells'...\n\n Calling function 'encode_categorical_variables'...\nCleansing done. This is some basic information about the data: \n \n There are 195 rows in the dataframe and 0.0% have a NA-value in the column 'OBS_RAW_VALUE\n\n \n This is the summary of the column 'TIME_PERIOD': count     195.000000\nmean     2014.061538\nstd         0.606076\nmin      2014.000000\n25%      2014.000000\n50%      2014.000000\n75%      2014.000000\nmax      2020.000000\nName: TIME_PERIOD, dtype: float64\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-337d54c67ee2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# Normalizing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     dataframe_normalized = scaler.normalizer(\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mcleansed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataframe_cleansed\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0msql_subset_query_string\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"DIMENSION_VALUES_NORMALIZATION\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/workspaces/data-etl/normalize/scaler.py\u001b[0m in \u001b[0;36mnormalizer\u001b[0;34m(cleansed_data, sql_subset_query_string, variable_type, is_inverted, whisker_factor, raw_data_col, scaled_data_col_name, maximum_score, log_info)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Define the normalization type, e.g. 0-5-10, or 0 - 3.3 - 6.6 - 10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mdistance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaximum_score\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlength_unique_values\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdivisor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength_unique_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# 1. Create a flat file of all WPA sources\n",
    "# Read and join all world policy analysis centre data\n",
    "wpa_child_labor = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_8, S_9' / 'WORLD_child_labor.xls'\n",
    ")\n",
    "\n",
    "wpa_childhood = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_10, S_13, S_36, S_45, S_49' / 'WORLD_Dataset_Childhood_4.16.15.xls'\n",
    ")\n",
    "\n",
    "wpa_adult_labor = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_40, S_41, S_63, S_64, S_65, S_66, S_67, S_68' / 'WORLD_Dataset_Adult_Labor_9.17.2018.xls'\n",
    ")\n",
    "\n",
    "wpa_discrimination = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_42, S_43, S_44' / 'WORLD_discrimination_at_work.xls'\n",
    ")\n",
    "\n",
    "# Create list to write a loop\n",
    "wpa_combined_list=[\n",
    "    wpa_childhood,\n",
    "    wpa_adult_labor,\n",
    "    wpa_discrimination\n",
    " ]\n",
    "\n",
    "# Loop to join all dataframes\n",
    "wpa_combined = wpa_child_labor\n",
    "\n",
    "for df in wpa_combined_list:\n",
    "    wpa_combined = wpa_combined.merge(\n",
    "        right=df,\n",
    "        on=['iso2', 'iso3']\n",
    "    )\n",
    "\n",
    "# 2. Loop\n",
    "\n",
    "# WPA sources\n",
    "wpa_sources = crba_data_dictionary_source[\n",
    "    (crba_data_dictionary_source[\"SOURCE_BODY\"] == \"World Policy Analysis Centre\")\n",
    "].merge(\n",
    "    right = crba_data_dictionary_snapshot,\n",
    "    on = \"SOURCE_ID\"\n",
    ").merge(\n",
    "    right = crba_data_dictionary_indicator,\n",
    "    on = 'INDICATOR_ID'\n",
    ")\n",
    "\n",
    "# # # # # # # Temp, delete this again # # # # \n",
    "wpa_sources = wpa_sources[wpa_sources[\"SOURCE_ID\"] == 'S-49']\n",
    "combined_cleansed_csv = pd.DataFrame()\n",
    "combined_normalized_csv = pd.DataFrame()\n",
    "\n",
    "# Loop to extract data from API sources\n",
    "for index, row in wpa_sources.iterrows():\n",
    "    # Log\n",
    "    print(\"\\n - - - - - \\n Extracting source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Exraction section\n",
    "    #try:\n",
    "    # Extract data \n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n Extracting data and store it as raw data\")\n",
    "\n",
    "    dataframe = wpa_combined[['iso3', row['WPA_OBS_RAW_COL']]] \n",
    "    dataframe['TIME_PERIOD'] = row['WPA_YEAR_COL'] \n",
    "\n",
    "    # Save dataframe\n",
    "    dataframe.to_csv(\n",
    "        data_sources_raw / str(row[\"SOURCE_ID\"] + \"_raw.csv\"),\n",
    "        sep = \";\")\n",
    "    #except:\n",
    "    print(\"There was an issue with source {}\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Log that we are entering cleasning\n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "    \n",
    "    # Cleansing \n",
    "    print(\"\\n - - - - - \\n Cleansing source {} \\n\".format(row[\"SOURCE_ID\"]))\n",
    "\n",
    "    dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "        raw_data=dataframe,\n",
    "        mapping_dictionary=mapping_dict,\n",
    "        final_sdmx_col_list=sdmx_df_columns_all\n",
    "    )\n",
    "\n",
    "    print(dataframe['RAW_OBS_VALUE'].unique())\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_and_discard_countries(\n",
    "        grouped_data=dataframe,\n",
    "        crba_country_list=country_crba_list,\n",
    "        country_list_full = country_full_list\n",
    "    )\n",
    "\n",
    "    dataframe = cleanse.Cleanser().add_cols_fill_cells(\n",
    "        grouped_data_iso_filt=dataframe,\n",
    "        dim_cols=sdmx_df_columns_dims,\n",
    "        time_cols=sdmx_df_columns_time,\n",
    "        indicator_name_string=row[\"INDICATOR_NAME_x\"],\n",
    "        index_name_string=row[\"INDEX\"],\n",
    "        issue_name_string=row[\"ISSUE\"],\n",
    "        category_name_string=row[\"CATEGORY\"],\n",
    "        indicator_code_string=row[\"INDICATOR_CODE\"],\n",
    "        indicator_source_string=row[\"ADDRESS\"],\n",
    "        indicator_source_body_string=row[\"SOURCE_BODY\"],\n",
    "        indicator_description_string=row[\"INDICATOR_DESCRIPTION\"],\n",
    "        indicator_explanation_string=row[\"INDICATOR_EXPLANATION\"],\n",
    "        indicator_data_extraction_methodology_string=row[\"EXTRACTION_METHODOLOGY\"],\n",
    "        source_title_string=row[\"SOURCE_TITLE\"],\n",
    "        source_api_link_string=row[\"ENDPOINT_URL\"]\n",
    "    )\n",
    "\n",
    "    dataframe_cleansed = cleanse.Cleanser().encode_categorical_variables(\n",
    "        dataframe = dataframe,\n",
    "        encoding_string = row[\"VALUE_ENCODING\"],\n",
    "        encoding_labels = row[\"VALUE_LABELS\"]\n",
    "    )\n",
    "\n",
    "    cleanse.Cleanser().create_log_report(\n",
    "        cleansed_data=dataframe_cleansed\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_cleansed_csv = combined_cleansed_csv.append(\n",
    "        other = dataframe_cleansed\n",
    "    )\n",
    "    \n",
    "\n",
    "    # Save cleansed data\n",
    "    dataframe_cleansed.to_csv(\n",
    "        data_sources_cleansed / str(row[\"SOURCE_ID\"] + \"_cleansed.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Normalizing\n",
    "    dataframe_normalized = scaler.normalizer(\n",
    "        cleansed_data = dataframe_cleansed,\n",
    "        sql_subset_query_string=row[\"DIMENSION_VALUES_NORMALIZATION\"],\n",
    "        # dim_cols=sdmx_df_columns_dims,\n",
    "        variable_type = row[\"VALUE_LABELS\"],\n",
    "        is_inverted = row[\"INVERT_NORMALIZATION\"],\n",
    "        whisker_factor=1.5,\n",
    "        raw_data_col=\"RAW_OBS_VALUE\",\n",
    "        scaled_data_col_name=\"SCALED_OBS_VALUE\",\n",
    "        maximum_score=10,\n",
    "        )\n",
    "\n",
    "    dataframe_normalized.to_csv(\n",
    "        data_sources_normalized / str(row[\"SOURCE_ID\"] + \"_normalized.csv\"),\n",
    "        sep = \";\")\n",
    "\n",
    "    # Append dataframe to combined dataframe\n",
    "    combined_normalized_csv = combined_normalized_csv.append(\n",
    "        other = dataframe_normalized\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n Calling function 'encode_categorical_variables'...\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    iso3  edu_comp_begsec  TIME_PERIOD RAW_OBS_VALUE  \\\n",
       "0    AFG              5.0         2019             2   \n",
       "1    ALB              5.0         2019             2   \n",
       "2    DZA              5.0         2019             2   \n",
       "3    AND              5.0         2019             2   \n",
       "4    AGO              1.0         2019             1   \n",
       "..   ...              ...          ...           ...   \n",
       "188  VEN              5.0         2019             2   \n",
       "189  VNM              1.0         2019             1   \n",
       "190  YEM              5.0         2019             2   \n",
       "191  ZMB              1.0         2019             1   \n",
       "192  ZWE              1.0         2019             1   \n",
       "\n",
       "                               ATTR_ENCODING_LABELS  \n",
       "0    2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "1    2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "2    2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "3    2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "4    2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "..                                              ...  \n",
       "188  2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "189  2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "190  2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "191  2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "192  2 = Compulsory; 1= Not compulsory; 0 = No data  \n",
       "\n",
       "[193 rows x 5 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>iso3</th>\n      <th>edu_comp_begsec</th>\n      <th>TIME_PERIOD</th>\n      <th>RAW_OBS_VALUE</th>\n      <th>ATTR_ENCODING_LABELS</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>AFG</td>\n      <td>5.0</td>\n      <td>2019</td>\n      <td>2</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ALB</td>\n      <td>5.0</td>\n      <td>2019</td>\n      <td>2</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DZA</td>\n      <td>5.0</td>\n      <td>2019</td>\n      <td>2</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AND</td>\n      <td>5.0</td>\n      <td>2019</td>\n      <td>2</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>AGO</td>\n      <td>1.0</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>188</th>\n      <td>VEN</td>\n      <td>5.0</td>\n      <td>2019</td>\n      <td>2</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>189</th>\n      <td>VNM</td>\n      <td>1.0</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>190</th>\n      <td>YEM</td>\n      <td>5.0</td>\n      <td>2019</td>\n      <td>2</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>191</th>\n      <td>ZMB</td>\n      <td>1.0</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n    <tr>\n      <th>192</th>\n      <td>ZWE</td>\n      <td>1.0</td>\n      <td>2019</td>\n      <td>1</td>\n      <td>2 = Compulsory; 1= Not compulsory; 0 = No data</td>\n    </tr>\n  </tbody>\n</table>\n<p>193 rows × 5 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# Read and join all world policy analysis centre data\n",
    "wpa_child_labor = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_8, S_9' / 'WORLD_child_labor.xls'\n",
    ")\n",
    "\n",
    "wpa_childhood = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_10, S_13, S_36, S_45, S_49' / 'WORLD_Dataset_Childhood_4.16.15.xls'\n",
    ")\n",
    "\n",
    "wpa_adult_labor = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_40, S_41, S_63, S_64, S_65, S_66, S_67, S_68' / 'WORLD_Dataset_Adult_Labor_9.17.2018.xls'\n",
    ")\n",
    "\n",
    "wpa_discrimination = pd.read_excel(\n",
    "    io = data_in / 'data_raw_manually_extracted' / 'S_42, S_43, S_44' / 'WORLD_discrimination_at_work.xls'\n",
    ")\n",
    "\n",
    "# Create list to write a loop\n",
    "wpa_combined_list=[\n",
    "    wpa_childhood,\n",
    "    wpa_adult_labor,\n",
    "    wpa_discrimination\n",
    " ]\n",
    "\n",
    "# Loop to join all dataframes\n",
    "wpa_combined = wpa_child_labor\n",
    "\n",
    "for df in wpa_combined_list:\n",
    "    wpa_combined = wpa_combined.merge(\n",
    "        right=df,\n",
    "        on=['iso2', 'iso3']\n",
    "    )\n",
    "\n",
    "s8_raw = wpa_combined[['iso3', 'admiss_age']]\n",
    "s8_raw['TIME_PERIOD'] = 2019\n",
    "\n",
    "s9_raw = wpa_combined[['iso3', 'light_age']]\n",
    "s9_raw['TIME_PERIOD'] = 2019\n",
    "\n",
    "s13_raw = wpa_combined[['iso3', 'cl_haz_minage']]\n",
    "s13_raw['TIME_PERIOD'] = 2014\n",
    "\n",
    "s36_raw = wpa_combined[['iso3', 'minwage_leg']]\n",
    "s36_raw['TIME_PERIOD'] = 2014\n",
    "\n",
    "s41_raw = wpa_combined[['iso3', 'sickleave_duration']]\n",
    "s41_raw['TIME_PERIOD'] = 2014\n",
    "\n",
    "s42_raw = wpa_combined[['iso3', 'promdemo_sex']]\n",
    "s42_raw['TIME_PERIOD'] = 2014\n",
    "\n",
    "s67_raw = wpa_combined[['iso3', 'paternal_leave']]\n",
    "s67_raw['TIME_PERIOD'] = 2014\n",
    "# # # # \n",
    "\n",
    "\n",
    "s10_raw = wpa_combined[['iso3', 'edu_comp_begsec']]\n",
    "s10_raw['TIME_PERIOD'] = 2019\n",
    "\n",
    "s10_raw\n",
    "\n",
    "dataframe_cleansed = cleanse.Cleanser().encode_categorical_variables(\n",
    "    dataframe = s10_raw,\n",
    "    encoding_string = '2 = 5.0; 1= 1.0',\n",
    "    na_encodings='No data',\n",
    "    obs_raw_value_source='edu_comp_begsec',\n",
    "    obs_raw_value_target=\"RAW_OBS_VALUE\",\n",
    "    encoding_labels='2 = Compulsory; 1= Not compulsory; 0 = No data'\n",
    ")\n",
    "\n",
    "# dataframe_cleansed['RAW_OBS_VALUE'].unique()\n",
    "dataframe_cleansed\n",
    "\n",
    "\n",
    "# # # ## \n",
    "\"\"\"\n",
    "s40_raw = wpa_combined[['iso3', 'paid_anlv']]\n",
    "s40_raw['TIME_PERIOD'] = 2019\n",
    "\n",
    "s40_raw\n",
    "\n",
    "dataframe_cleansed = cleanse.Cleanser().encode_categorical_variables(\n",
    "    dataframe = s40_raw,\n",
    "    encoding_string = '20 days or more=5.0; 15-19 days=4.0; Set externally=3.0; 5-9 days = 2.0; No paid annual leave = 1.0',\n",
    "    na_encodings='No data',\n",
    "    obs_raw_value_source='paid_anlv',\n",
    "    obs_raw_value_target=\"RAW_OBS_VALUE\"\n",
    ")\n",
    "\n",
    "dataframe_cleansed['RAW_OBS_VALUE'].unique()\n",
    "\"\"\"\n",
    "dataframe_cleansed"
   ]
  },
  {
   "source": [
    "## GHG\n",
    "\n",
    "problem: \n",
    "\n",
    "* Cannot use pd_json_normalize --> It won't unnest it becaust he years and actual values are given in [] rather than {}\n",
    "* Stopped here: Must replace the character (but for that it is necessary to convert json to string and back)\n",
    "* Next challnge: Get all data (and not only page 1) --> Main problem is that the API is not working, i.e. can't specify paramters\n",
    "\n",
    "Stopped here 10.11.20 --> Need to get rid of \"[]\" signgs for "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_json = requests.get(\"https://www.climatewatchdata.org/api/v1/data/historical_emissions\").text\n",
    "\n",
    "dataframe_json_cleansed = dataframe_json.replace('[', '{').replace(']','}')\n",
    "json_file = json.dumps(dataframe_json_cleansed)\n",
    "# some_var = json.loads(dataframe_json_cleansed)\n",
    "# type(some_var)\n",
    "# pd.json_normalize(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'id': 518203,\n",
       " 'iso_code3': 'AFG',\n",
       " 'country': 'Afghanistan',\n",
       " 'data_source': 'CAIT',\n",
       " 'sector': 'Total excluding LUCF',\n",
       " 'gas': 'CH4',\n",
       " 'unit': 'MtCO₂e',\n",
       " 'emissions': [{'year': 1990, 'value': 8.97},\n",
       "  {'year': 1991, 'value': 9.07},\n",
       "  {'year': 1992, 'value': 9.0},\n",
       "  {'year': 1993, 'value': 8.9},\n",
       "  {'year': 1994, 'value': 8.97},\n",
       "  {'year': 1995, 'value': 9.15},\n",
       "  {'year': 1996, 'value': 9.93},\n",
       "  {'year': 1997, 'value': 10.6},\n",
       "  {'year': 1998, 'value': 11.1},\n",
       "  {'year': 1999, 'value': 11.87},\n",
       "  {'year': 2000, 'value': 10.59},\n",
       "  {'year': 2001, 'value': 9.36},\n",
       "  {'year': 2002, 'value': 11.21},\n",
       "  {'year': 2003, 'value': 11.56},\n",
       "  {'year': 2004, 'value': 11.47},\n",
       "  {'year': 2005, 'value': 11.68},\n",
       "  {'year': 2006, 'value': 14.89},\n",
       "  {'year': 2007, 'value': 18.1},\n",
       "  {'year': 2008, 'value': 22.19},\n",
       "  {'year': 2009, 'value': 25.43},\n",
       "  {'year': 2010, 'value': 30.04},\n",
       "  {'year': 2011, 'value': 39.48},\n",
       "  {'year': 2012, 'value': 48.78},\n",
       "  {'year': 2013, 'value': 58.13},\n",
       "  {'year': 2014, 'value': 67.77},\n",
       "  {'year': 2015, 'value': 76.62},\n",
       "  {'year': 2016, 'value': 78.17}]}"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "#dataframe = extract.CSVExtractor.extract(url = \"https://www.climatewatchdata.org/api/v1/data/historical_emissions\")\n",
    "\n",
    "dataframe_json = requests.get(\"https://www.climatewatchdata.org/api/v1/data/historical_emissions\").json()\n",
    "\n",
    "cleaned_json = dataframe_json['data']# .replace('[', '{').replace\n",
    "type(cleaned_json)\n",
    "\"\"\"\n",
    "# with open(dataframe_json, 'r') as file:\n",
    "    content = file.read()\n",
    "    clean = content.replace(']', '}')  # cleanup here\n",
    "    json_data = json.loads(clean)\n",
    "\"\"\"\n",
    "\n",
    "# dataframe_2 = pd.json_normalize(dataframe_json['data'], max_level=5)\n",
    "\n",
    "# dataframe_3 = pd.json_normalize(dataframe_json['data)\n",
    "\n",
    "\n",
    "#dataframe_3\n",
    "\n",
    "cleaned_json[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4 as bs\n",
    "import pandas\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Specify location of chromedriver\n",
    "cwd = os.getcwd()\n",
    "driver_location = cwd + '\\\\chromedriver.exe'\n",
    "\n",
    "# Add option to make it headless (so that it doesn't open an actual chrome window)\n",
    "options = Options()\n",
    "options.headless = True\n",
    "driver = webdriver.Chrome(driver_location, chrome_options=options)\n",
    "\n",
    "# Get HTTP response\n",
    "# response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO:11300:P11300_INSTRUMENT_ID:312256:NO\")\n",
    "response = driver.get(\"https://www.ilo.org/dyn/normlex/en/f?p=NORMLEXPUB:11300:0::NO::P11300_INSTRUMENT_ID:312283\")\n",
    "\n",
    "# Get response\n",
    "# response = driver.get(html_url)\n",
    "\n",
    "# Retrieve the actual html\n",
    "html = driver.page_source\n",
    "\n",
    "# Soupify\n",
    "soup = bs.BeautifulSoup(html)\n",
    "\n",
    "# Extract the target table as attribute\n",
    "target_table = str(\n",
    "    soup.find_all(\"table\", {\"cellspacing\": \"0\", \"class\": \"horizontalLine\"})\n",
    ")\n",
    "\n",
    "# Create dataframe with the data\n",
    "raw_data = pd.read_html(io=target_table, header=0)[\n",
    "    0\n",
    "]  # return is a list of DFs, specify [0] to get actual DF\n",
    "\n",
    "# Cleansing\n",
    "dataframe = cleanse.Cleanser().rename_and_discard_columns(\n",
    "    raw_data=raw_data,\n",
    "    mapping_dictionary=mapping_dict,\n",
    "    final_sdmx_col_list=sdmx_df_columns_all\n",
    ")\n",
    "\n",
    "# dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(country_full_list.loc[subset_list, \"COUNTRY_NAME\"].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_full_list.COUNTRY_NAME[47] in dataframe.COUNTRY_NAME[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_full_list.COUNTRY_NAME[47]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe[\"COUNTRY_NAME\"][20]\n",
    "\n",
    "[re.search(x+'*?', dataframe.COUNTRY_NAME[20]) for x in country_full_list.COUNTRY_NAME]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "temp = extract_country_name(dataframe[\"COUNTRY_NAME\"][20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataframe[\"COUNTRY_NAME\"].apply(extract_country_name).sample(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_country_name(cell, country_name_list = country_full_list.COUNTRY_NAME):\n",
    "    # Determine which country in the full country list is contained in string\n",
    "    subset_list = [x in cell for x in country_name_list]\n",
    "    \n",
    "    # Sometimes several country names match, but we need exactly one\n",
    "    if sum(subset_list)==0:\n",
    "        print(\"No country name match\")\n",
    "    elif sum(subset_list)==1:\n",
    "        # Retrieve the actual country name\n",
    "        country_name = country_name_list[subset_list].item()\n",
    "    else:\n",
    "        # Retrieve the actual country name\n",
    "        country_name = country_name_list[subset_list].iloc[0]\n",
    "    \n",
    "    return country_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_full_list.COUNTRY_NAME[subset_list].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "country_full_list\n",
    "    \n",
    "subset_list = [x in dataframe.COUNTRY_NAME[2] for x in country_full_list.COUNTRY_NAME]\n",
    "\n",
    "dataframe.COUNTRY_NAME[2] = country_full_list.loc[subset_list, \"COUNTRY_NAME\"].item()\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract UN Treaty data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "import bs4 as bs\n",
    "import pandas as pd\n",
    "\n",
    "raw_html_1 = requests.get(\"https://treaties.un.org/pages/ViewDetails.aspx?src=TREATY&mtdsg_no=XVIII-12-a&chapter=18&clang=_en\")\n",
    "\n",
    "# raw_html_1.text\n",
    "raw_html_2 = urllib.request.urlopen(\"https://treaties.un.org/pages/ViewDetails.aspx?src=TREATY&mtdsg_no=XVIII-12-a&chapter=18&clang=_en\")\n",
    "\n",
    "print(raw_html_1)\n",
    "print(raw_html_2)\n",
    "\n",
    "soup = bs.BeautifulSoup(raw_html_1.text, features=\"lxml\")\n",
    "#soup_2 = bs.BeautifulSoup(raw_html_2, features=\"lxml\")\n",
    "\n",
    "#soup_2\n",
    "# soup_1\n",
    "\n",
    "# Extract the target table as attribute\n",
    "target_table = str(\n",
    "    soup.find_all(\n",
    "        \"table\",\n",
    "        {\"class\": \"table table-striped table-bordered table-hover table-condensed\"},\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create dataframe with the data\n",
    "raw_data = pd.read_html(io=target_table, header=0)[\n",
    "    0\n",
    "]\n",
    "\n",
    "raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify duplicates\n",
    "\n",
    "# Checking to see if there are any duplicate entries\n",
    "duplicates = combined_normalized_csv[combined_normalized_csv.duplicated(\n",
    "    subset = [\n",
    "        \"COUNTRY_ISO_3\",\n",
    "        \"TIME_PERIOD\",\n",
    "        \"COUNTRY_NAME\",\n",
    "        \"COUNTRY_ISO_2\",\n",
    "        \"RAW_OBS_VALUE\",\n",
    "        \"DIM_SEX\",\n",
    "        \"DIM_EDU_LEVEL\",\n",
    "        \"DIM_AGE\",\n",
    "        \"DIM_AGE_GROUP\",\n",
    "        \"DIM_MANAGEMENT_LEVEL\",\n",
    "        \"DIM_AREA_TYPE\",\n",
    "        \"DIM_QUANTILE\",\n",
    "        \"DIM_SDG_INDICATOR\",\n",
    "        \"DIM_OCU_TYPE\",\n",
    "        \"DIM_REP_TYPE\",\n",
    "        \"DIM_SECTOR\",\n",
    "        \"INDICATOR_CODE\"\n",
    "        ],\n",
    "    keep = False\n",
    "\n",
    ")]\n",
    "\n",
    "duplicates.to_csv(\n",
    "    path_or_buf = cwd / 'data_out' / 'duplicates.csv',\n",
    "    sep = \";\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}