{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data-extraction-CRBA\" data-toc-modified-id=\"Data-extraction-CRBA-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data extraction CRBA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Backlog-of-tasks\" data-toc-modified-id=\"Backlog-of-tasks-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Backlog of tasks</a></span></li><li><span><a href=\"#Import-packages\" data-toc-modified-id=\"Import-packages-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Import packages</a></span></li><li><span><a href=\"#Import-country-list\" data-toc-modified-id=\"Import-country-list-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Import country list</a></span></li></ul></li><li><span><a href=\"#Data-extraction\" data-toc-modified-id=\"Data-extraction-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data extraction</a></span><ul class=\"toc-item\"><li><span><a href=\"#S-11-Economist-Intelligence-Unit,-Out-of-the-Shadows-Index.-Legal-Framework-score-only\" data-toc-modified-id=\"S-11-Economist-Intelligence-Unit,-Out-of-the-Shadows-Index.-Legal-Framework-score-only-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>S-11 Economist Intelligence Unit, Out of the Shadows Index. Legal Framework score only</a></span></li><li><span><a href=\"#S-55-(prev-S-15)-Percentage-of-out-of-school-adolescents-of-lower-secondary-school-age.\" data-toc-modified-id=\"S-55-(prev-S-15)-Percentage-of-out-of-school-adolescents-of-lower-secondary-school-age.-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>S-55 (prev S-15) Percentage of out-of-school adolescents of lower secondary school age.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.2.1\"><span class=\"toc-item-num\">2.2.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-2.2.2\"><span class=\"toc-item-num\">2.2.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.2.3\"><span class=\"toc-item-num\">2.2.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-2.2.4\"><span class=\"toc-item-num\">2.2.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li><li><span><a href=\"#S-56-(prev-S-16)-Percentage-of-out-of-school-youth-of-upper-secondary-school-age\" data-toc-modified-id=\"S-56-(prev-S-16)-Percentage-of-out-of-school-youth-of-upper-secondary-school-age-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>S-56 (prev S-16) Percentage of out-of-school youth of upper secondary school age</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.3.1\"><span class=\"toc-item-num\">2.3.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-2.3.2\"><span class=\"toc-item-num\">2.3.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.3.3\"><span class=\"toc-item-num\">2.3.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-2.3.4\"><span class=\"toc-item-num\">2.3.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li><li><span><a href=\"#S-24-(prev.-S-14)-SDG-Indicator-8.7.1-Proportion-of-children-aged-5-17-years-engaged-in-child-labour-SL_TLF_CHLDEA\" data-toc-modified-id=\"S-24-(prev.-S-14)-SDG-Indicator-8.7.1-Proportion-of-children-aged-5-17-years-engaged-in-child-labour-SL_TLF_CHLDEA-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>S-24 (prev. S-14) SDG Indicator 8.7.1 Proportion of children aged 5-17 years engaged in child labour SL_TLF_CHLDEA</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.4.1\"><span class=\"toc-item-num\">2.4.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-2.4.2\"><span class=\"toc-item-num\">2.4.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.4.3\"><span class=\"toc-item-num\">2.4.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-2.4.4\"><span class=\"toc-item-num\">2.4.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li><li><span><a href=\"#S-23-(prev.-S-17)-ILO-STAT-Informal-Employment-(%-of-total-non-agricultural-employment)-SL_ISV_IFEM\" data-toc-modified-id=\"S-23-(prev.-S-17)-ILO-STAT-Informal-Employment-(%-of-total-non-agricultural-employment)-SL_ISV_IFEM-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>S-23 (prev. S-17) ILO STAT Informal Employment (% of total non-agricultural employment) SL_ISV_IFEM</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.5.1\"><span class=\"toc-item-num\">2.5.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleansing\" data-toc-modified-id=\"Cleansing-2.5.2\"><span class=\"toc-item-num\">2.5.2&nbsp;&nbsp;</span>Cleansing</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.5.3\"><span class=\"toc-item-num\">2.5.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Select-only-those-rows/-columns-which-are-relevant\" data-toc-modified-id=\"Select-only-those-rows/-columns-which-are-relevant-2.5.4\"><span class=\"toc-item-num\">2.5.4&nbsp;&nbsp;</span>Select only those rows/ columns which are relevant</a></span></li></ul></li><li><span><a href=\"#S-60-(prev-S-18)-Walk-Free-Foundation.Global-Slavery-Index.-Prevalence-of-Modern-Slavery.-Prevalence-score-only.\" data-toc-modified-id=\"S-60-(prev-S-18)-Walk-Free-Foundation.Global-Slavery-Index.-Prevalence-of-Modern-Slavery.-Prevalence-score-only.-2.6\"><span class=\"toc-item-num\">2.6&nbsp;&nbsp;</span>S-60 (prev S-18) Walk Free Foundation.Global Slavery Index. Prevalence of Modern Slavery. Prevalence score only.</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.6.1\"><span class=\"toc-item-num\">2.6.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Transform\" data-toc-modified-id=\"Transform-2.6.2\"><span class=\"toc-item-num\">2.6.2&nbsp;&nbsp;</span>Transform</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.6.3\"><span class=\"toc-item-num\">2.6.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-2.6.4\"><span class=\"toc-item-num\">2.6.4&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li></ul></li><li><span><a href=\"#S-61-(prev-S-19)-SDG-Indicator-16.2.2-Detected-victims-of-human-trafficking,-by-age-and-sex-(number)--VC_HTF_DETV\" data-toc-modified-id=\"S-61-(prev-S-19)-SDG-Indicator-16.2.2-Detected-victims-of-human-trafficking,-by-age-and-sex-(number)--VC_HTF_DETV-2.7\"><span class=\"toc-item-num\">2.7&nbsp;&nbsp;</span>S-61 (prev S-19) SDG Indicator 16.2.2 Detected victims of human trafficking, by age and sex (number)  VC_HTF_DETV</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.7.1\"><span class=\"toc-item-num\">2.7.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Cleanse\" data-toc-modified-id=\"Cleanse-2.7.2\"><span class=\"toc-item-num\">2.7.2&nbsp;&nbsp;</span>Cleanse</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.7.3\"><span class=\"toc-item-num\">2.7.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-2.7.4\"><span class=\"toc-item-num\">2.7.4&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li></ul></li><li><span><a href=\"#S-62-(prev-S-20)-SDG-Indicator-1.1.1.-Proportion-of-population-below-international-poverty-line-(%)-SI_POV_DAY1\" data-toc-modified-id=\"S-62-(prev-S-20)-SDG-Indicator-1.1.1.-Proportion-of-population-below-international-poverty-line-(%)-SI_POV_DAY1-2.8\"><span class=\"toc-item-num\">2.8&nbsp;&nbsp;</span>S-62 (prev S-20) SDG Indicator 1.1.1. Proportion of population below international poverty line (%) SI_POV_DAY1</a></span><ul class=\"toc-item\"><li><span><a href=\"#Extraction\" data-toc-modified-id=\"Extraction-2.8.1\"><span class=\"toc-item-num\">2.8.1&nbsp;&nbsp;</span>Extraction</a></span></li><li><span><a href=\"#Transformation\" data-toc-modified-id=\"Transformation-2.8.2\"><span class=\"toc-item-num\">2.8.2&nbsp;&nbsp;</span>Transformation</a></span></li><li><span><a href=\"#Normalization-(scaling)\" data-toc-modified-id=\"Normalization-(scaling)-2.8.3\"><span class=\"toc-item-num\">2.8.3&nbsp;&nbsp;</span>Normalization (scaling)</a></span></li><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-2.8.4\"><span class=\"toc-item-num\">2.8.4&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li></ul></li></ul></li><li><span><a href=\"#---------Archive/-Trash/-dev-section-(Disregard-this-section)-----------\" data-toc-modified-id=\"---------Archive/-Trash/-dev-section-(Disregard-this-section)------------3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span><div align=\"center\"> - - - - Archive/ Trash/ dev section (Disregard this section) - - - - - </div></a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li><li><span><a href=\"#Left-join-to-counry-list-to-bring-it-into-target-format\" data-toc-modified-id=\"Left-join-to-counry-list-to-bring-it-into-target-format-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Left join to counry list to bring it into target format</a></span></li></ul></li><li><span><a href=\"#Define-function\" data-toc-modified-id=\"Define-function-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Define function</a></span><ul class=\"toc-item\"><li><span><a href=\"#Generic-functions\" data-toc-modified-id=\"Generic-functions-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;</span>Generic functions</a></span></li><li><span><a href=\"#UNESCO-API-specific-functions\" data-toc-modified-id=\"UNESCO-API-specific-functions-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;</span>UNESCO API specific functions</a></span></li><li><span><a href=\"#SDG-API-specific-functions\" data-toc-modified-id=\"SDG-API-specific-functions-3.1.3\"><span class=\"toc-item-num\">3.1.3&nbsp;&nbsp;</span>SDG API specific functions</a></span><ul class=\"toc-item\"><li><span><a href=\"#Transforming-ILO-data\" data-toc-modified-id=\"Transforming-ILO-data-3.1.3.1\"><span class=\"toc-item-num\">3.1.3.1&nbsp;&nbsp;</span>Transforming ILO data</a></span></li><li><span><a href=\"#Extracting-data-through-ILO-API\" data-toc-modified-id=\"Extracting-data-through-ILO-API-3.1.3.2\"><span class=\"toc-item-num\">3.1.3.2&nbsp;&nbsp;</span>Extracting data through ILO API</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction CRBA\n",
    "\n",
    "* This is the main file, which must be executed to extract data from all soures for which the process has been automated\n",
    "* For a list of all soures along with the information whether its a manual or atutomated extraction please consult the follownig file: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backlog of tasks\n",
    "\n",
    " - [X] Write the functions to extract data from UNESCO API into one function which takes the indicator code as argument (rather than having two functions with the same functionatlity for two indicators | and check if that ingo is not also in the SDG API anyways\n",
    " - [ ] Finalize understanding the maplecroft logic for a quantaitive indicator and then for: qualitative indicator and also understand all the exceptions (.e.g inverted indicator etc.) --> Draw a decision tree of all if-then statements and check if this logic is desired by Alex and Tomas\n",
    " - [ ] Implement this logic in the python function started below\n",
    " - [ ] Check with Alex/ Tomas what the final result should look like (raw data + scaled data + aggregated data = final index score for each indicator + aggregrated index scores\n",
    " - [ ] Check with Daniele in what format these things should be in SDMX (should they include dimensions like sex, age, ... SHould they include units etc.?\n",
    " - [ ] Start implementing INdicator by indicator (rather than extracting data from all sources first before transforming anything)\n",
    " - [ ] For log file: \n",
    "    * Summary statistics of indicator\n",
    "    * Coverage (how many NA values) for indicator\n",
    "    * Data on average from which year \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import urllib\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "from extract.unesco_extractor import extract_unesco_api_data\n",
    "from extract.sdg_extractor import extract_sdg_api_data\n",
    "from extract import save_raw_data\n",
    "from cleanse.unesco_cleanser import cleanse_unesco_api_data\n",
    "from cleanse.sdg_cleanser import cleanse_sdg_api_data\n",
    "from cleanse.save_cleansed_data import save_cleansed_data\n",
    "from normalize import scaler\n",
    "from normalize import save_normalized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import country list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the export path for all RAW data sources\n",
    "cwd = os.getcwd()\n",
    "\n",
    "data_sources_raw = cwd + \"\\data\\data_raw\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list of countries which contains all different variations of country names \n",
    "country_full_list = pd.read_excel(cwd + '\\\\all_countrynames_list.xlsx',\n",
    "                                 keep_default_na = False).drop_duplicates()\n",
    "\n",
    "# Create a version of the list with unique ISO2 and ISO3 codes\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'CountryIso2')\n",
    "\n",
    "# Country CRBA list, this is the list of the countries that should be contained \n",
    "country_crba_list = pd.read_excel(cwd + '\\\\crba_country_list.xlsx',\n",
    "                                 header = None,\n",
    "                                 usecols = [0, 1], \n",
    "                                 names = ['COUNTRY_ISO_3', 'COUNTRY_NAME']).merge(\n",
    "                                        right = country_iso_list,\n",
    "                                        how = 'left',\n",
    "                                        left_on = 'COUNTRY_ISO_3',\n",
    "                                        right_on = 'CountryIso3',\n",
    "                                        validate = 'one_to_one')[\n",
    "            ['COUNTRY_ISO_3', 'COUNTRY_NAME', 'CountryIso2']].rename(columns = {'CountryIso2': \"COUNTRY_ISO_2\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-11 Economist Intelligence Unit, Out of the Shadows Index. Legal Framework score only\n",
    "\n",
    "I have decided to not automate this process, because the Excel is so nested that it is very difficult to do that. And it's faster and less erorr prone to retrieve the data manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-55 (prev S-15) Percentage of out-of-school adolescents of lower secondary school age. \n",
    "\n",
    "The data of this source is retrieved from the UNSECO API, for which you must have an API key to acces it. For that, you must sign up. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s55_raw = extract_unesco_api_data(\n",
    "    api_call_url = 'https://api.uis.unesco.org/sdmx/data/UNESCO,SDG4,2.0/ROFST.PT.L2+L2_3+L3._T._T+F+M.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod=2005&endPeriod=2018&format=csv-sdmx&locale=en&subscription-key=460ab272abdd43c892bb59c218c22c09'\n",
    ")\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s55_raw,\n",
    "             filename = 'S_55_raw.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain latest available observation and discard countries that are not in the final CRBA country list \n",
    "s55_cleansed = cleanse_unesco_api_data(raw_data = s55_raw,\n",
    "                                   raw_data_iso_2_col = 'REF_AREA',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['OBS_VALUE', 'TIME_PERIOD', 'OBS_STATUS']\n",
    "                                   ) # the variable OBS_STATUS varies, but is an attribute, not a dimension. It specifies how an OBS_VALUE was created (A = standard way, E = estimate)\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s55_cleansed,\n",
    "             filename = 'S_55_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s55_normalized = scaler.normalizer(cleansed_data = s55_cleansed,\n",
    "                        indicator_raw_value = 'OBS_VALUE',\n",
    "                        indicator_code = '3.1.3',\n",
    "                        indicator_name = 'Out-of-school adolescents (lower secondary)',\n",
    "                        cleansed_df_iso2_col = 'REF_AREA',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['TIME_PERIOD', \n",
    "                                        'REF_AREA', \n",
    "                                        'OBS_VALUE', \n",
    "                                        'OBS_STATUS', \n",
    "                                        'COUNTRY_ISO_3', \n",
    "                                        'COUNTRY_NAME', \n",
    "                                        'COUNTRY_ISO_2', \n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s55_normalized,\n",
    "             filename = 'S_55_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-56 (prev S-16) Percentage of out-of-school youth of upper secondary school age\n",
    "\n",
    "The data of this source is retrieved from the UNSECO API, for which you must have an API key to acces it. For that, you must sign up. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s56_raw = extract_unesco_api_data(\n",
    "    api_call_url = 'https://api.uis.unesco.org/sdmx/data/UNESCO,SDG4,2.0/ROFST.PT.L2+L2_3+L3._T._T+F+M.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod=2005&endPeriod=2018&format=csv-sdmx&locale=en&subscription-key=460ab272abdd43c892bb59c218c22c09'\n",
    ")\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s56_raw,\n",
    "             filename = 'S_56_raw.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain latest available observation and discard countries that are not in the final CRBA country list \n",
    "s56_cleansed = cleanse_unesco_api_data(raw_data = s56_raw,\n",
    "                                   raw_data_iso_2_col = 'REF_AREA',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['OBS_VALUE', \n",
    "                                                   'TIME_PERIOD', \n",
    "                                                   'OBS_STATUS']\n",
    "                                   ) # the variable OBS_STATUS varies, but is an attribute, not a dimension. It specifies how an OBS_VALUE was created (A = standard way, E = estimate)\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s56_cleansed,\n",
    "             filename = 'S_56_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s56_normalized = scaler.normalizer(cleansed_data = s56_cleansed,\n",
    "                        indicator_raw_value = 'OBS_VALUE',\n",
    "                        indicator_code = '3.1.4',\n",
    "                        indicator_name = 'Out-of-school adolescents (upper secondary)',\n",
    "                        cleansed_df_iso2_col = 'REF_AREA',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['TIME_PERIOD',\n",
    "                                        'REF_AREA',\n",
    "                                        'OBS_VALUE',\n",
    "                                        'OBS_STATUS',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s56_normalized,\n",
    "             filename = 'S_56_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-24 (prev. S-14) SDG Indicator 8.7.1 Proportion of children aged 5-17 years engaged in child labour SL_TLF_CHLDEA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted raw data contains the following columns: Index(['goal', 'target', 'indicator', 'series', 'seriesDescription',\n",
      "       'seriesCount', 'geoAreaCode', 'geoAreaName', 'timePeriodStart', 'value',\n",
      "       'valueType', 'time_detail', 'timeCoverage', 'upperBound', 'lowerBound',\n",
      "       'basePeriod', 'source', 'geoInfoUrl', 'footnotes', 'attributes.Nature',\n",
      "       'attributes.Units', 'dimensions.Age', 'dimensions.Sex',\n",
      "       'dimensions.Reporting Type'],\n",
      "      dtype='object')\n",
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_raw\\\n"
     ]
    }
   ],
   "source": [
    "# Extract data and directly flatten it out into a pandas dataframe\n",
    "s24_raw = extract_sdg_api_data(series_code = 'SL_TLF_CHLDEA')\n",
    "\n",
    "# Save data to raw data folder\n",
    "save_raw_data.save_raw_data(dataframe = s24_raw,\n",
    "             filename = 'S_24.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_cleansed\\\n"
     ]
    }
   ],
   "source": [
    "s24_cleansed = cleanse_sdg_api_data(raw_data = s24_raw,\n",
    "                                   country_list_full = country_full_list,\n",
    "                                   country_list_full_name_col = 'CountryDesc',\n",
    "                                   country_list_full_iso2_col = 'CountryIso2',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['value',\n",
    "                                                  'source',\n",
    "                                                   'timePeriodStart'\n",
    "                                                  'footnotes',\n",
    "                                                  'Unnamed: 0'] \n",
    "                                   )\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s24_cleansed,\n",
    "             filename = 'S_24_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You hve a selected a few columns, which will not be regarded as dimensions. These are the remaining columns in the dataset, along with the number of values they take in the dataset.\n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column source has 1 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Age has 5 unique values.\n",
      "The column dimensions.Sex has 3 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "The total number of subgroups in the dataset is therefore: 15\n",
      "In the loop we are currently dealing with the subset #1, which has these defining values:     goal   target  indicator         series  \\\n",
      "0  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "0  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "0        None         None       None       None       None   \n",
      "\n",
      "                        source geoInfoUrl attributes.Nature attributes.Units  \\\n",
      "0  UNICEF and ILO calculations       None                CA          PERCENT   \n",
      "\n",
      "  dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "0           5-17           MALE                         G  \n",
      "\n",
      " The shape of the subset is: (62, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 38.46138. This value corresponds to country: 48    Cameroon\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 0.39296. This value corresponds to country: 247    Turkmenistan\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #2, which has these defining values:     goal   target  indicator         series  \\\n",
      "2  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "2  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "2        None         None       None       None       None   \n",
      "\n",
      "                        source geoInfoUrl attributes.Nature attributes.Units  \\\n",
      "2  UNICEF and ILO calculations       None                CA          PERCENT   \n",
      "\n",
      "  dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "2           5-17         FEMALE                         G  \n",
      "\n",
      " The shape of the subset is: (62, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 33.36696. This value corresponds to country: 54    Chad\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 0.11869. This value corresponds to country: 246    Turkmenistan\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #3, which has these defining values:     goal   target  indicator         series  \\\n",
      "1  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "1  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "1        None         None       None       None       None   \n",
      "\n",
      "                        source geoInfoUrl attributes.Nature attributes.Units  \\\n",
      "1  UNICEF and ILO calculations       None                CA          PERCENT   \n",
      "\n",
      "  dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "1           5-17        BOTHSEX                         G  \n",
      "\n",
      " The shape of the subset is: (62, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 33.96166. This value corresponds to country: 118    Guinea-Bissau\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 0.26104. This value corresponds to country: 248    Turkmenistan\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #4, which has these defining values:      goal   target  indicator         series  \\\n",
      "20  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                    seriesDescription seriesCount valueType  \\\n",
      "20  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "   time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "20        None         None       None       None       None   \n",
      "\n",
      "                         source geoInfoUrl attributes.Nature attributes.Units  \\\n",
      "20  UNICEF and ILO calculations       None                CA          PERCENT   \n",
      "\n",
      "   dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "20           5-14         FEMALE                         G  \n",
      "\n",
      " The shape of the subset is: (21, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 30.0706. This value corresponds to country: 40    Burkina Faso\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 0.79932. This value corresponds to country: 240    Trinidad and Tobago\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #5, which has these defining values:      goal   target  indicator         series  \\\n",
      "19  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                    seriesDescription seriesCount valueType  \\\n",
      "19  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "   time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "19        None         None       None       None       None   \n",
      "\n",
      "                         source geoInfoUrl attributes.Nature attributes.Units  \\\n",
      "19  UNICEF and ILO calculations       None                CA          PERCENT   \n",
      "\n",
      "   dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "19           5-14        BOTHSEX                         G  \n",
      "\n",
      " The shape of the subset is: (21, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 35.11702. This value corresponds to country: 39    Burkina Faso\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 0.74341. This value corresponds to country: 242    Trinidad and Tobago\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #6, which has these defining values:      goal   target  indicator         series  \\\n",
      "18  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                    seriesDescription seriesCount valueType  \\\n",
      "18  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "   time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "18        None         None       None       None       None   \n",
      "\n",
      "                         source geoInfoUrl attributes.Nature attributes.Units  \\\n",
      "18  UNICEF and ILO calculations       None                CA          PERCENT   \n",
      "\n",
      "   dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "18           5-14           MALE                         G  \n",
      "\n",
      " The shape of the subset is: (21, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 40.13007. This value corresponds to country: 41    Burkina Faso\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 0.69124. This value corresponds to country: 241    Trinidad and Tobago\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #7, which has these defining values:      goal   target  indicator         series  \\\n",
      "35  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                    seriesDescription seriesCount valueType  \\\n",
      "35  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "   time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "35        None         None       None       None       None   \n",
      "\n",
      "                         source geoInfoUrl attributes.Nature attributes.Units  \\\n",
      "35  UNICEF and ILO calculations       None                CA          PERCENT   \n",
      "\n",
      "   dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "35           7-17           MALE                         G  \n",
      "\n",
      " The shape of the subset is: (4, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 17.4. This value corresponds to country: 109    Ghana\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 2.4. This value corresponds to country: 194          Panama\n",
      "236        Suriname\n",
      "226    South Africa\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #8, which has these defining values:      goal   target  indicator         series  \\\n",
      "33  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                    seriesDescription seriesCount valueType  \\\n",
      "33  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "   time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "33        None         None       None       None       None   \n",
      "\n",
      "                         source geoInfoUrl attributes.Nature attributes.Units  \\\n",
      "33  UNICEF and ILO calculations       None                CA          PERCENT   \n",
      "\n",
      "   dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "33           7-17         FEMALE                         G  \n",
      "\n",
      " The shape of the subset is: (4, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 17.6. This value corresponds to country: 110    Ghana\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 1.3. This value corresponds to country: 225    South Africa\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #9, which has these defining values:      goal   target  indicator         series  \\\n",
      "34  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                    seriesDescription seriesCount valueType  \\\n",
      "34  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "   time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "34        None         None       None       None       None   \n",
      "\n",
      "                         source geoInfoUrl attributes.Nature attributes.Units  \\\n",
      "34  UNICEF and ILO calculations       None                CA          PERCENT   \n",
      "\n",
      "   dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "34           7-17        BOTHSEX                         G  \n",
      "\n",
      " The shape of the subset is: (4, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 17.5. This value corresponds to country: 72     Cte d'Ivoire\n",
      "108            Ghana\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 1.9. This value corresponds to country: 132            Iraq\n",
      "227    South Africa\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #10, which has these defining values:       goal   target  indicator         series  \\\n",
      "158  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                     seriesDescription seriesCount valueType  \\\n",
      "158  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "    time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "158        None         None       None       None       None   \n",
      "\n",
      "                          source geoInfoUrl attributes.Nature  \\\n",
      "158  UNICEF and ILO calculations       None                CA   \n",
      "\n",
      "    attributes.Units dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "158          PERCENT           6-17        BOTHSEX                         G  \n",
      "\n",
      " The shape of the subset is: (2, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 10.4. This value corresponds to country: 158          Mali\n",
      "159    Mauritania\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 3.5. This value corresponds to country: 201    Philippines\n",
      "204         Rwanda\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #11, which has these defining values:       goal   target  indicator         series  \\\n",
      "157  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                     seriesDescription seriesCount valueType  \\\n",
      "157  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "    time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "157        None         None       None       None       None   \n",
      "\n",
      "                          source geoInfoUrl attributes.Nature  \\\n",
      "157  UNICEF and ILO calculations       None                CA   \n",
      "\n",
      "    attributes.Units dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "157          PERCENT           6-17           MALE                         G  \n",
      "\n",
      " The shape of the subset is: (2, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 12.9. This value corresponds to country: 157    Mali\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 3.9. This value corresponds to country: 14     Armenia\n",
      "205     Rwanda\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #12, which has these defining values:       goal   target  indicator         series  \\\n",
      "156  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                     seriesDescription seriesCount valueType  \\\n",
      "156  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "    time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "156        None         None       None       None       None   \n",
      "\n",
      "                          source geoInfoUrl attributes.Nature  \\\n",
      "156  UNICEF and ILO calculations       None                CA   \n",
      "\n",
      "    attributes.Units dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "156          PERCENT           6-17         FEMALE                         G  \n",
      "\n",
      " The shape of the subset is: (2, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 7.7. This value corresponds to country: 156    Mali\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 3.2. This value corresponds to country: 26     Belize\n",
      "192    Panama\n",
      "206    Rwanda\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #13, which has these defining values:       goal   target  indicator         series  \\\n",
      "177  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                     seriesDescription seriesCount valueType  \\\n",
      "177  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "    time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "177        None         None       None       None       None   \n",
      "\n",
      "                          source geoInfoUrl attributes.Nature  \\\n",
      "177  UNICEF and ILO calculations       None                CA   \n",
      "\n",
      "    attributes.Units dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "177          PERCENT          10-17           MALE                         G  \n",
      "\n",
      " The shape of the subset is: (2, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 33.3. This value corresponds to country: 177    Nicaragua\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 5.1. This value corresponds to country: 163         Mexico\n",
      "203    Philippines\n",
      "189       Pakistan\n",
      "260        Uruguay\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #14, which has these defining values:       goal   target  indicator         series  \\\n",
      "178  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                     seriesDescription seriesCount valueType  \\\n",
      "178  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "    time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "178        None         None       None       None       None   \n",
      "\n",
      "                          source geoInfoUrl attributes.Nature  \\\n",
      "178  UNICEF and ILO calculations       None                CA   \n",
      "\n",
      "    attributes.Units dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "178          PERCENT          10-17         FEMALE                         G  \n",
      "\n",
      " The shape of the subset is: (2, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 17.9. This value corresponds to country: 28         Benin\n",
      "178    Nicaragua\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 12.4. This value corresponds to country: 190    Pakistan\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #15, which has these defining values:       goal   target  indicator         series  \\\n",
      "179  ['8']  ['8.7']  ['8.7.1']  SL_TLF_CHLDEA   \n",
      "\n",
      "                                     seriesDescription seriesCount valueType  \\\n",
      "179  Proportion of children engaged in economic act...         273     Float   \n",
      "\n",
      "    time_detail timeCoverage upperBound lowerBound basePeriod  \\\n",
      "179        None         None       None       None       None   \n",
      "\n",
      "                          source geoInfoUrl attributes.Nature  \\\n",
      "179  UNICEF and ILO calculations       None                CA   \n",
      "\n",
      "    attributes.Units dimensions.Age dimensions.Sex dimensions.Reporting Type  \n",
      "179          PERCENT          10-17        BOTHSEX                         G  \n",
      "\n",
      " The shape of the subset is: (2, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 25.9. This value corresponds to country: 179    Nicaragua\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 9.0. This value corresponds to country: 33      Bolivia\n",
      "191    Pakistan\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_normalized\\\n"
     ]
    }
   ],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s24_normalized = scaler.normalizer(cleansed_data = s24_cleansed,\n",
    "                        indicator_raw_value = 'value',\n",
    "                        indicator_code = '3.1.1.',\n",
    "                        indicator_name = 'Child labour rate (5-17)',\n",
    "                        cleansed_df_iso2_col = 'CountryIso2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['geoAreaCode',\n",
    "                                        'geoAreaName',\n",
    "                                        'timePeriodStart',\n",
    "                                        'value',\n",
    "                                        'footnotes', \n",
    "                                        'Unnamed: 0', \n",
    "                                        'CountryDesc',\n",
    "                                        'CountryIso2',\n",
    "                                        'CountryIso3',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s24_normalized,\n",
    "             filename = 'S_24_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-23 (prev. S-17) ILO STAT Informal Employment (% of total non-agricultural employment) SL_ISV_IFEM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction\n",
    "\n",
    "This data can be extracted from two different sources: \n",
    "    \n",
    "* ILO\n",
    "* SDG database\n",
    "\n",
    "After consultation with Alex and Toms we have decided to extract the data from the SDG API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted raw data contains the following columns: Index(['goal', 'target', 'indicator', 'series', 'seriesDescription',\n",
      "       'seriesCount', 'geoAreaCode', 'geoAreaName', 'timePeriodStart', 'value',\n",
      "       'valueType', 'time_detail', 'timeCoverage', 'upperBound', 'lowerBound',\n",
      "       'basePeriod', 'source', 'geoInfoUrl', 'footnotes', 'attributes.Nature',\n",
      "       'attributes.Units', 'dimensions.Sex', 'dimensions.Reporting Type',\n",
      "       'dimensions.Activity'],\n",
      "      dtype='object')\n",
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_raw\\\n"
     ]
    }
   ],
   "source": [
    "# Extract data and directly flatten it out into a pandas dataframe\n",
    "s23_raw = extract_sdg_api_data(series_code = 'SL_ISV_IFEM')\n",
    "\n",
    "# Save data to raw data folder\n",
    "save_raw_data.save_raw_data(dataframe = s23_raw,\n",
    "             filename = 'S_23.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_cleansed\\\n"
     ]
    }
   ],
   "source": [
    "s23_cleansed = cleanse_sdg_api_data(raw_data = s23_raw,\n",
    "                                   country_list_full = country_full_list,\n",
    "                                   country_list_full_name_col = 'CountryDesc',\n",
    "                                   country_list_full_iso2_col = 'CountryIso2',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['value',\n",
    "                                                  'source',\n",
    "                                                  'footnotes',\n",
    "                                                   'timePeriodStart',\n",
    "                                                  'Unnamed: 0'] \n",
    "                                   )\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s23_cleansed,\n",
    "             filename = 'S_23_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You hve a selected a few columns, which will not be regarded as dimensions. These are the remaining columns in the dataset, along with the number of values they take in the dataset.\n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Sex has 3 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "The column dimensions.Activity has 3 unique values.\n",
      "The total number of subgroups in the dataset is therefore: 9\n",
      "In the loop we are currently dealing with the subset #1, which has these defining values:     goal   target  indicator       series  \\\n",
      "8  ['8']  ['8.3']  ['8.3.1']  SL_ISV_IFEM   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "8  Proportion of informal employment, by sector a...        3213     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod geoInfoUrl  \\\n",
      "8        None         None       None       None       None       None   \n",
      "\n",
      "  attributes.Nature attributes.Units dimensions.Sex dimensions.Reporting Type  \\\n",
      "8                 C          PERCENT        BOTHSEX                         G   \n",
      "\n",
      "  dimensions.Activity  \n",
      "8             ISIC4_A  \n",
      "\n",
      " The shape of the subset is: (69, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 99.78322. This value corresponds to country: 575    Tanzania\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data values for this subgroup contains outliers or is too skewed on the lower end. The minimum value to be used for the normalisation is 1st quartile or distribution - 1.5 * IQR. It is: 74.44202999999999 \n",
      " See histogram printed below for info. \n",
      "\n",
      "\n",
      " This is the distribution of the raw data of the indicator.\n",
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #2, which has these defining values:     goal   target  indicator       series  \\\n",
      "7  ['8']  ['8.3']  ['8.3.1']  SL_ISV_IFEM   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "7  Proportion of informal employment, by sector a...        3213     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod geoInfoUrl  \\\n",
      "7        None         None       None       None       None       None   \n",
      "\n",
      "  attributes.Nature attributes.Units dimensions.Sex dimensions.Reporting Type  \\\n",
      "7                 C          PERCENT        BOTHSEX                         G   \n",
      "\n",
      "  dimensions.Activity  \n",
      "7              NONAGR  \n",
      "\n",
      " The shape of the subset is: (69, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 96.25492. This value corresponds to country: 171    DR Congo\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 13.97051. This value corresponds to country: 487    Serbia\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #3, which has these defining values:     goal   target  indicator       series  \\\n",
      "0  ['8']  ['8.3']  ['8.3.1']  SL_ISV_IFEM   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "0  Proportion of informal employment, by sector a...        3213     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod geoInfoUrl  \\\n",
      "0        None         None       None       None       None       None   \n",
      "\n",
      "  attributes.Nature attributes.Units dimensions.Sex dimensions.Reporting Type  \\\n",
      "0                 C          PERCENT           MALE                         G   \n",
      "\n",
      "  dimensions.Activity  \n",
      "0               TOTAL  \n",
      "\n",
      " The shape of the subset is: (69, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 97.91753. This value corresponds to country: 173    DR Congo\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 19.12724. This value corresponds to country: 492    Serbia\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #4, which has these defining values:     goal   target  indicator       series  \\\n",
      "5  ['8']  ['8.3']  ['8.3.1']  SL_ISV_IFEM   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "5  Proportion of informal employment, by sector a...        3213     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod geoInfoUrl  \\\n",
      "5        None         None       None       None       None       None   \n",
      "\n",
      "  attributes.Nature attributes.Units dimensions.Sex dimensions.Reporting Type  \\\n",
      "5                 C          PERCENT         FEMALE                         G   \n",
      "\n",
      "  dimensions.Activity  \n",
      "5              NONAGR  \n",
      "\n",
      " The shape of the subset is: (69, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 97.87779. This value corresponds to country: 177    DR Congo\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 12.20581. This value corresponds to country: 490    Serbia\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #5, which has these defining values:     goal   target  indicator       series  \\\n",
      "4  ['8']  ['8.3']  ['8.3.1']  SL_ISV_IFEM   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "4  Proportion of informal employment, by sector a...        3213     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod geoInfoUrl  \\\n",
      "4        None         None       None       None       None       None   \n",
      "\n",
      "  attributes.Nature attributes.Units dimensions.Sex dimensions.Reporting Type  \\\n",
      "4                 C          PERCENT         FEMALE                         G   \n",
      "\n",
      "  dimensions.Activity  \n",
      "4             ISIC4_A  \n",
      "\n",
      " The shape of the subset is: (69, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 100.0. This value corresponds to country: 326    Maldives\n",
      "595       Yemen\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data values for this subgroup contains outliers or is too skewed on the lower end. The minimum value to be used for the normalisation is 1st quartile or distribution - 1.5 * IQR. It is: 84.22612000000001 \n",
      " See histogram printed below for info. \n",
      "\n",
      "\n",
      " This is the distribution of the raw data of the indicator.\n",
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #6, which has these defining values:     goal   target  indicator       series  \\\n",
      "3  ['8']  ['8.3']  ['8.3.1']  SL_ISV_IFEM   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "3  Proportion of informal employment, by sector a...        3213     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod geoInfoUrl  \\\n",
      "3        None         None       None       None       None       None   \n",
      "\n",
      "  attributes.Nature attributes.Units dimensions.Sex dimensions.Reporting Type  \\\n",
      "3                 C          PERCENT         FEMALE                         G   \n",
      "\n",
      "  dimensions.Activity  \n",
      "3               TOTAL  \n",
      "\n",
      " The shape of the subset is: (69, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 99.30467. This value corresponds to country: 176    DR Congo\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 20.67644. This value corresponds to country: 489    Serbia\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #7, which has these defining values:     goal   target  indicator       series  \\\n",
      "2  ['8']  ['8.3']  ['8.3.1']  SL_ISV_IFEM   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "2  Proportion of informal employment, by sector a...        3213     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod geoInfoUrl  \\\n",
      "2        None         None       None       None       None       None   \n",
      "\n",
      "  attributes.Nature attributes.Units dimensions.Sex dimensions.Reporting Type  \\\n",
      "2                 C          PERCENT           MALE                         G   \n",
      "\n",
      "  dimensions.Activity  \n",
      "2             ISIC4_A  \n",
      "\n",
      " The shape of the subset is: (69, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 99.82906. This value corresponds to country: 311    Liberia\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data values for this subgroup contains outliers or is too skewed on the lower end. The minimum value to be used for the normalisation is 1st quartile or distribution - 1.5 * IQR. It is: 70.734985 \n",
      " See histogram printed below for info. \n",
      "\n",
      "\n",
      " This is the distribution of the raw data of the indicator.\n",
      "AxesSubplot(0.125,0.125;0.775x0.755)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #8, which has these defining values:     goal   target  indicator       series  \\\n",
      "1  ['8']  ['8.3']  ['8.3.1']  SL_ISV_IFEM   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "1  Proportion of informal employment, by sector a...        3213     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod geoInfoUrl  \\\n",
      "1        None         None       None       None       None       None   \n",
      "\n",
      "  attributes.Nature attributes.Units dimensions.Sex dimensions.Reporting Type  \\\n",
      "1                 C          PERCENT           MALE                         G   \n",
      "\n",
      "  dimensions.Activity  \n",
      "1              NONAGR  \n",
      "\n",
      " The shape of the subset is: (69, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 95.32092. This value corresponds to country: 174    DR Congo\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 15.4177. This value corresponds to country: 493    Serbia\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the loop we are currently dealing with the subset #9, which has these defining values:     goal   target  indicator       series  \\\n",
      "6  ['8']  ['8.3']  ['8.3.1']  SL_ISV_IFEM   \n",
      "\n",
      "                                   seriesDescription seriesCount valueType  \\\n",
      "6  Proportion of informal employment, by sector a...        3213     Float   \n",
      "\n",
      "  time_detail timeCoverage upperBound lowerBound basePeriod geoInfoUrl  \\\n",
      "6        None         None       None       None       None       None   \n",
      "\n",
      "  attributes.Nature attributes.Units dimensions.Sex dimensions.Reporting Type  \\\n",
      "6                 C          PERCENT        BOTHSEX                         G   \n",
      "\n",
      "  dimensions.Activity  \n",
      "6               TOTAL  \n",
      "\n",
      " The shape of the subset is: (69, 32)\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is 98.61896. This value corresponds to country: 179    DR Congo\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n",
      "The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is 19.80673. This value corresponds to country: 486    Serbia\n",
      "Name: COUNTRY_NAME, dtype: object \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py:231: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_normalized\\\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s23_normalized = scaler.normalizer(cleansed_data = s23_cleansed,\n",
    "                        indicator_raw_value = 'value',\n",
    "                        indicator_code = '3.1.5',\n",
    "                        indicator_name = 'Informal employment.',\n",
    "                        cleansed_df_iso2_col = 'CountryIso2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['geoAreaCode',\n",
    "                                        'geoAreaName',\n",
    "                                        'timePeriodStart',\n",
    "                                        'source',\n",
    "                                        'value',\n",
    "                                        'footnotes', \n",
    "                                        'Unnamed: 0', \n",
    "                                        'CountryDesc',\n",
    "                                        'CountryIso2',\n",
    "                                        'CountryIso3',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s23_normalized,\n",
    "             filename = 'S_23_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only those rows/ columns which are relevant \n",
    "\n",
    "* < to do >\n",
    "* Right now, the normalized dataset contains data on ALL subsets (which are defined by the dimension values). Ultimately, there has to be a commitment to one Subset of an indicator \n",
    "* But this filtering to exactly one dimension (i.e. one row per country) will be done later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-60 (prev S-18) Walk Free Foundation.Global Slavery Index. Prevalence of Modern Slavery. Prevalence score only.\n",
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "url = 'http://downloads.globalslaveryindex.org/ephemeral/FINAL-GSI-2018-DATA-G20-AND-FISHING-1597151668.xlsx'\n",
    "\n",
    "s60_raw = pd.read_excel(io = url, \n",
    "                      sheet_name = 'Global prev, vuln, govt table',\n",
    "                       skiprows = 2)\n",
    "\n",
    "# Save data to raw data folder\n",
    "save_raw_data.save_raw_data(dataframe = s60_raw,\n",
    "             filename = 'S_60.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter relevant columns\n",
    "s60_raw = s60_raw[['Country ', # FYI: note the trailing space after Country\n",
    "        'Est. prevalence of population in modern slavery (victims per 1,000 population)']]\n",
    "\n",
    "# The data is from 2018, and is therefore assumed to be reflecting the prevalen in the year 2018\n",
    "s60_cleansed = s60_raw.assign(Year = 2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and \n",
    "s60_scaled = s60_cleansed.assign(\n",
    "    scaled = scaler(s60_cleansed['Est. prevalence of population in modern slavery (victims per 1,000 population)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s60_final = country_full_list.merge(right = s60_scaled,\n",
    "                 indicator = True,\n",
    "                 how = 'inner',\n",
    "                 left_on = 'CountryDesc',\n",
    "                 right_on = 'Country ',\n",
    "                 validate = 'many_to_one').assign(indicator ='I-18')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. This difference is explicable because there were already missing values for the value in sxx_ scaled dataset'.format(\n",
    "    len(s60_scaled), \n",
    "    s60_final['Est. prevalence of population in modern slavery (victims per 1,000 population)'].notna().sum(), \n",
    "    len(s60_scaled) - s60_final['Est. prevalence of population in modern slavery (victims per 1,000 population)'].notna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-61 (prev S-19) SDG Indicator 16.2.2 Detected victims of human trafficking, by age and sex (number)  VC_HTF_DETV \n",
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The extracted raw data contains the following columns: Index(['goal', 'target', 'indicator', 'series', 'seriesDescription',\n",
      "       'seriesCount', 'geoAreaCode', 'geoAreaName', 'timePeriodStart', 'value',\n",
      "       'valueType', 'time_detail', 'timeCoverage', 'upperBound', 'lowerBound',\n",
      "       'basePeriod', 'source', 'geoInfoUrl', 'footnotes', 'attributes.Nature',\n",
      "       'attributes.Units', 'dimensions.Age', 'dimensions.Sex',\n",
      "       'dimensions.Reporting Type'],\n",
      "      dtype='object')\n",
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_raw\\\n"
     ]
    }
   ],
   "source": [
    "# Extract data\n",
    "s61_raw = extract_sdg_api_data(series_code = 'VC_HTF_DETV')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data.save_raw_data(dataframe = s61_raw,\n",
    "              filename = 'S_61.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data has been saved as .xlsx file in: D:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\data\\data_cleansed\\\n"
     ]
    }
   ],
   "source": [
    "s61_cleansed = cleanse_sdg_api_data(raw_data = s61_raw,\n",
    "                                   country_list_full = country_full_list,\n",
    "                                   country_list_full_name_col = 'CountryDesc',\n",
    "                                   country_list_full_iso2_col = 'CountryIso2',\n",
    "                                   country_df = country_crba_list,\n",
    "                                   country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "                                   non_dim_cols = ['value',\n",
    "                                                  'source',\n",
    "                                                  'footnotes',\n",
    "                                                   'timePeriodStart',\n",
    "                                                  'Unnamed: 0'] \n",
    "                                   )\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s61_cleansed,\n",
    "             filename = 'S_61_cleansed.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # # #STOPPED HERE --> GOT TO DEBUG THE SCALER FUNCTION SO THAT IT CAN ALSO DEAL WITH DATAFRAMES WITH ONLY ONE DIMENSION-Country-group (U believe that is where the bug is coming from)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You hve a selected a few columns, which will not be regarded as dimensions. These are the remaining columns in the dataset, along with the number of values they take in the dataset.\n",
      "The column goal has 1 unique values.\n",
      "The column target has 1 unique values.\n",
      "The column indicator has 1 unique values.\n",
      "The column series has 1 unique values.\n",
      "The column seriesDescription has 1 unique values.\n",
      "The column seriesCount has 1 unique values.\n",
      "The column valueType has 1 unique values.\n",
      "The column time_detail has 1 unique values.\n",
      "The column timeCoverage has 1 unique values.\n",
      "The column upperBound has 1 unique values.\n",
      "The column lowerBound has 1 unique values.\n",
      "The column basePeriod has 1 unique values.\n",
      "The column geoInfoUrl has 1 unique values.\n",
      "The column attributes.Nature has 1 unique values.\n",
      "The column attributes.Units has 1 unique values.\n",
      "The column dimensions.Age has 1 unique values.\n",
      "The column dimensions.Sex has 1 unique values.\n",
      "The column dimensions.Reporting Type has 1 unique values.\n",
      "The total number of subgroups in the dataset is therefore: 1\n",
      "In the loop we are currently dealing with the subset #1, which has these defining values: Empty DataFrame\n",
      "Columns: [goal, target, indicator, series, seriesDescription, seriesCount, valueType, time_detail, timeCoverage, upperBound, lowerBound, basePeriod, geoInfoUrl, attributes.Nature, attributes.Units, dimensions.Age, dimensions.Sex, dimensions.Reporting Type]\n",
      "Index: []\n",
      "\n",
      " The shape of the subset is: (0, 32)\n",
      "Dataframe is empty. There are no values to append.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"The following 'value_vars' are not present in the DataFrame: ['SCALED']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-46b9da7ed796>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m                                         \u001b[1;34m'COUNTRY_ISO_3'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                                         \u001b[1;34m'COUNTRY_NAME'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                                         '_merge']\n\u001b[0m\u001b[0;32m     24\u001b[0m                        )\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Documents\\2020\\28_UNICEF\\10_working_repo\\data-etl\\normalize\\scaler.py\u001b[0m in \u001b[0;36mnormalizer\u001b[1;34m(cleansed_data, indicator_raw_value, indicator_code, indicator_name, cleansed_df_iso2_col, crba_final_country_list, crba_final_country_list_iso_col, cat_var, cat_scoring_type, inverted, non_dim_cols, whisker_factor)\u001b[0m\n\u001b[0;32m    261\u001b[0m                \u001b[0mid_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkept_columns\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m                \u001b[0mvalue_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindicator_raw_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'SCALED'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m                var_name = 'VALUE_TYPE')\n\u001b[0m\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;31m# Assign indicator code and name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\michael\\anaconda3\\envs\\unicef-test\\lib\\site-packages\\pandas\\core\\reshape\\melt.py\u001b[0m in \u001b[0;36mmelt\u001b[1;34m(frame, id_vars, value_vars, var_name, value_name, col_level)\u001b[0m\n\u001b[0;32m     76\u001b[0m                     \u001b[1;34m\"The following 'value_vars' are not present in \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                     \u001b[1;34m\"the DataFrame: {missing}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m                     \u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m                 )\n\u001b[0;32m     80\u001b[0m         \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid_vars\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mvalue_vars\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"The following 'value_vars' are not present in the DataFrame: ['SCALED']\""
     ]
    }
   ],
   "source": [
    "# Scale the raw data and bring the dataframe into long formmat\n",
    "s61_normalized = scaler.normalizer(cleansed_data = s61_cleansed,\n",
    "                        indicator_raw_value = 'value',\n",
    "                        indicator_code = '3.2.2',\n",
    "                        indicator_name = 'Prevalence of human trafficking',\n",
    "                        cleansed_df_iso2_col = 'CountryIso2',\n",
    "                        crba_final_country_list = country_crba_list,\n",
    "                        crba_final_country_list_iso_col = 'COUNTRY_ISO_2',\n",
    "                        inverted = True,\n",
    "                        non_dim_cols = ['geoAreaCode',\n",
    "                                        'geoAreaName',\n",
    "                                        'timePeriodStart',\n",
    "                                        'source',\n",
    "                                        'value',\n",
    "                                        'footnotes', \n",
    "                                        'Unnamed: 0', \n",
    "                                        'CountryDesc',\n",
    "                                        'CountryIso2',\n",
    "                                        'CountryIso3',\n",
    "                                        'COUNTRY_ISO_2',\n",
    "                                        'COUNTRY_ISO_3',\n",
    "                                        'COUNTRY_NAME',\n",
    "                                        '_merge']\n",
    "                       )\n",
    "\n",
    "# save normalized data\n",
    "save_normalized_data.save_normalized_data(dataframe = s61_normalized,\n",
    "             filename = 'S_61_normalized.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s61_final = country_full_list.merge(right = s61_scaled,\n",
    "                 indicator = True,\n",
    "                 how = 'outer',\n",
    "                 left_on = 'CountryDesc',\n",
    "                 right_on = 'geoAreaName',\n",
    "                 validate = 'many_to_one').assign(indicator ='I-19')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. Therefore, {} rows got lost during the join.'.format(\n",
    "    len(s61_scaled), \n",
    "    s61_final.value.notna().sum(), \n",
    "    len(s61_scaled) - s61_final.value.notna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S-62 (prev S-20) SDG Indicator 1.1.1. Proportion of population below international poverty line (%) SI_POV_DAY1 \n",
    "### Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s62_raw = extract_sdg_api_data(series_code = 'SI_POV_DAY1')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data(dataframe = s62_raw,\n",
    "              filename = 'S_62.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the raw data\n",
    "s62_cleansed = transform_sdg_api_data(raw_data = s62_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization (scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and \n",
    "s62_scaled = s62_cleansed.assign(\n",
    "    scaled = scaler(s62_cleansed.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s62_final = country_full_list.merge(right = s62_scaled,\n",
    "                 indicator = True,\n",
    "                 how = 'outer',\n",
    "                 left_on = 'CountryDesc',\n",
    "                 right_on = 'geoAreaName', # validate = 'many_to_one'\n",
    "                 ).assign(indicator ='I-20')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. Therefore, {} rows got lost during the join.'.format(\n",
    "    len(s62_scaled), \n",
    "    s61_final.value.notna().sum(), \n",
    "    len(s62_scaled) - s62_final.value.notna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <div align=\"center\"> - - - - Archive/ Trash/ dev section (Disregard this section) - - - - - </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_dim_cols = ['value',\n",
    "              'source',\n",
    "              'timePeriodStart',\n",
    "              'footnotes',\n",
    "              'Unnamed: 0']\n",
    "\n",
    "debug_s23 = s23_raw[s23_raw['geoAreaName'] == 'Albania'][['geoAreaName',\n",
    "                                                         'timePeriodStart',\n",
    "                                                         'dimensions.Sex',\n",
    "                                                         'dimensions.Activity']]\n",
    "\n",
    "\n",
    "debug_col_list = debug_s23.columns.to_list() # list of all columns in the dataframe\n",
    "debug_non_dim_cols_tuple = tuple(non_dim_cols) # parameters must be passed as list, but the following command requires a tuple\n",
    "debug_col_list_gb = [e for e in debug_col_list if e not in debug_non_dim_cols_tuple] # exclude timePeriodStart and value, because these one'saren't used for the groupby statement\n",
    "\n",
    "    # Some of the columns contain values which Python interprets as lists (e.g. [8.1]). These make the groupby statement malfunction. Convert them to string\n",
    "debug_s23[debug_col_list_gb] =  debug_s23[debug_col_list_gb].astype(str)\n",
    "\n",
    "    # It may be that the time period column is not of type numerical\n",
    "debug_s23 = debug_s23.astype({'timePeriodStart': float})\n",
    "\n",
    "# convert column \"a\" to int64 dtype and \"b\" to complex type\n",
    "# df = df.astype({\"a\": int, \"b\": complex})\n",
    "\n",
    "    # Retreive the latest available data for each group, where group is 'col_list_gb'\n",
    "grouped_data = debug_s23[debug_s23['timePeriodStart'] == debug_s23.groupby(debug_col_list_gb)['timePeriodStart'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def SOME_CLEANSER(raw_data, country_list_full, country_list_full_name_col, country_list_full_iso2_col, country_df, country_df_iso2_col, non_dim_cols):\n",
    "    \"\"\"Cleanse raw data from SDG API\n",
    "\n",
    "    Retrieve only the latest observations for each country-dimension group. NB Dimensions make are part of the composite primary key of the dataset and therefore their values define groups, e.g. male vs femeale in a country.\n",
    "    Discard countries that aren't in the CRBA final list of countries\n",
    "\n",
    "    Parameters:\n",
    "    raw_data (obj): Return of function 'extract_sdg_api_data'. Should be a pandas dataframe.\n",
    "    country_list_full (obj): Dataframe containing the full list of countries with all their country name variations, should be in long format\n",
    "    country_list_full_name_col (str): Column in 'country_list_full', which contains the country names (this column should be the primary key of the dataframe as well)\n",
    "    country_list_full_iso2_col (str): Column in 'country_list_full', which contains the country iso2 codes.\n",
    "    country_df (obj): Dataframe containing the final CRBA list of countries\n",
    "    country_df_iso2_col (str): Column in 'country_df', which contains the country iso2 codes.\n",
    "    non_dim_cols (list): List of columns, which should be excluded from the group by statement (that is, if a column is not indicated here, then the maximum value for each of its values will be calculated)\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe, which only contains the countries meant to be in CRBA and the latest observed value for the indiator.\n",
    "\n",
    "   \"\"\"\n",
    "\n",
    "    # Group by to obtain the latest available value per group, where group is 'col_list_gb'\n",
    "        # Create list of all column to group by\n",
    "    col_list = raw_data.columns.to_list() # list of all columns in the dataframe\n",
    "    non_dim_cols_tuple = tuple(non_dim_cols) # parameters must be passed as list, but the following command requires a tuple\n",
    "    col_list_gb = [e for e in col_list if e not in non_dim_cols_tuple] # exclude timePeriodStart and value, because these one'saren't used for the groupby statement\n",
    "\n",
    "        # Some of the columns contain values which Python interprets as lists (e.g. [8.1]). These make the groupby statement malfunction. Convert them to string\n",
    "    raw_data[col_list_gb] =  raw_data[col_list_gb].astype(str)\n",
    "\n",
    "        # It may be that the time period column is not of type numerical\n",
    "    raw_data['timePeriodStart'] = pd.to_numeric(raw_data['timePeriodStart'])\n",
    "\n",
    "        # Retreive the latest available data for each group, where group is 'col_list_gb'\n",
    "    grouped_data = raw_data[raw_data[\n",
    "        'timePeriodStart'] == raw_data.groupby(col_list_gb)['timePeriodStart'].transform('max')]\n",
    "\n",
    "    \n",
    "    #print(raw_data.groupby(col_list_gb)['timePeriodStart'].transform('max').head(15))\n",
    "    #print(raw_data['timePeriodStart'] == raw_data.groupby(col_list_gb)['timePeriodStart'].transform('max'))\n",
    "    print(col_list_gb)\n",
    "    # print(grouped_data[grouped_data['geoAreaName'] == 'Angola'].head(50))\n",
    "    \n",
    "    # Discard rows of countries that are not in the master country list\n",
    "        # The raw data only contains country names. Assign ISO codes to these country names\n",
    "    grouped_data_iso = grouped_data.merge(\n",
    "        right = country_list_full,\n",
    "        how = 'left',\n",
    "        left_on = 'geoAreaName',\n",
    "        right_on = country_list_full_name_col,\n",
    "        validate = 'many_to_one')\n",
    "\n",
    "        # Discard countries that aren't part of the final CRBA master list\n",
    "    grouped_data_iso_filt = grouped_data_iso.merge(\n",
    "        right = country_df,\n",
    "        how = 'right',\n",
    "        left_on = country_list_full_iso2_col,\n",
    "        right_on = country_df_iso2_col,\n",
    "        indicator = True,\n",
    "        validate = 'many_to_one')\n",
    "\n",
    "    # Note of Michael: \"Validation: Number of rows should be 195\" --> This is incorrect, the number can also diverge (because the left table may contain one, several or no values for a country-key)\n",
    "\n",
    "    # return result\n",
    "    return(grouped_data_iso_filt.sort_values(by = country_df_iso2_col, axis = 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric(s23_normalized[\n",
    "    (s23_normalized['dimensions.Sex'] == 'BOTHSEX') &\n",
    "    (s23_normalized['dimensions.Activity'] == 'NONAGR') &\n",
    "    (s23_normalized['VALUE_TYPE'] == 'value') \n",
    "    \n",
    "]['value']).hist(bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tot_num_subsets = 1\n",
    "for col in s23_cleansed:\n",
    "    print('The column {} has {} unique values.'.format(col, s23_cleansed[col].nunique()))\n",
    "    tot_num_subsets *= s23_cleansed[col].nunique()\n",
    "print(tot_num_subsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the raw data and \n",
    "s23_scaled = s23_cleansed.assign(\n",
    "    scaled = scaler(s23_cleansed.value))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are intested in sector = non-agricultural and want the data for both sexes. Filter the other rows out:\n",
    "temp = s23_raw[(s23_raw['dimensions.Sex'] == 'BOTHSEX') & (s23_raw['dimensions.Activity'] == 'NONAGR')]\n",
    "\n",
    "# transform data\n",
    "s23_cleansed = transform_sdg_api_data(raw_data = temp)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s24_final = country_full_list.merge(right = s24_scaled,\n",
    "                 indicator = True,\n",
    "                 how = 'left',\n",
    "                 left_on = 'CountryDesc',\n",
    "                 right_on = 'geoAreaName',\n",
    "                 validate = 'many_to_one').assign(indicator ='I-14')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. Therefore, {} rows got lost during the join.'.format(\n",
    "    len(s24_scaled), \n",
    "    s24_final.value.notna().sum(), \n",
    "    len(s24_scaled) - s24_final.value.notna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "    \n",
    "def normalizer(cleansed_data, indicator_raw_value, cleansed_df_iso2_col, crba_final_country_list, crba_final_country_list_iso_col, cat_var = False, cat_scoring_type = None, inverted = False, non_dim_cols = None, whisker_factor = 1.5):\n",
    "    \"\"\" Transform cleansed datasets into scaled (normalized) data in long format\n",
    "    \n",
    "    Parameters:\n",
    "    cleansed_data (obj): Raw dataset, pandas dataframe\n",
    "    indicator_raw_value (str): Column containing the actual raw data values \n",
    "    cleansed_df_iso2_col (str): Column which holds the ISO2 code in 'cleansed_data'\n",
    "    crba_final_country_list (obj): Final CRBA country list, should contain country ISO2 (primary key), ISO3 and country name columns.\n",
    "    crba_final_country_list_iso_col (str): Column in crba_final_country_list which holds the ISO2 codes of the countries. \n",
    "    cat_var (bool): is variable of type categorical or numerical?\n",
    "    cat_scoring_type (str): For categorical variables, specify the scoring type (i.e. what values the categorical variable can take)\n",
    "    inverted (bool): If we are dealing with a numerical variable, should the values be inverted?\n",
    "    whisker_factor (float): Whisker factor which is multiplied with the interquartile range (IQR) to define outliers. Default value is 1.5, as is standard in statistics.\n",
    "    non_dim_cols (list): List of columns who are required to uniquely identify a row in the dataset. The normalization will be done for each dimension-subset there is.  \n",
    "\n",
    "    Returns:\n",
    "    obj: Returns numpy array with transformed values\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if an indicator is numeric of categorical\n",
    "    if cat_var == True:\n",
    "        # This is the section dealing with categorical indicators\n",
    "        # There are various scoring_types a categorical variable can take. For each one, different labels apply to the values\n",
    "        # Type 2-1-0\n",
    "        if cat_scoring_type == 'Type 2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 0),\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '10.00']\n",
    "\n",
    "        # Type 2-1\n",
    "        elif cat_scoring_type == 'Type 2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '10.00']\n",
    "\n",
    "        # Type 3-2-1\n",
    "        elif cat_scoring_type == 'Type 3-2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2),\n",
    "                (cleansed_data[indicator_raw_value] == 3)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '5.00', '10.00']\n",
    "\n",
    "        # Type 3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 0)\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2),\n",
    "                (cleansed_data[indicator_raw_value] == 3)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '5.00', '10.00']            \n",
    "            \n",
    "        # Type 4-3-2-1\n",
    "        elif cat_scoring_type == 'Type 4-3-2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2),\n",
    "                (cleansed_data[indicator_raw_value] == 3),\n",
    "                (cleansed_data[indicator_raw_value] == 4)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '3.33', '6.67', '10.00']                \n",
    "            \n",
    "            \n",
    "            # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "            cleansed_data['SCALED'] = np.select(conditions, values)\n",
    "\n",
    "        # Type 4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 0)\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2),\n",
    "                (cleansed_data[indicator_raw_value] == 3),\n",
    "                (cleansed_data[indicator_raw_value] == 4)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '3.33', '6.67', '10.00'] \n",
    "\n",
    "\n",
    "        # Type 5-4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 5-4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 0)\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2),\n",
    "                (cleansed_data[indicator_raw_value] == 3),\n",
    "                (cleansed_data[indicator_raw_value] == 4),\n",
    "                (cleansed_data[indicator_raw_value] == 5)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '2.50', '5.00', '7.50', '10.00']\n",
    "            \n",
    "        # Type 7-6-5-4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 7-6-5-4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (cleansed_data[indicator_raw_value] == 0)\n",
    "                (cleansed_data[indicator_raw_value] == 1),\n",
    "                (cleansed_data[indicator_raw_value] == 2),\n",
    "                (cleansed_data[indicator_raw_value] == 3),\n",
    "                (cleansed_data[indicator_raw_value] == 4),\n",
    "                (cleansed_data[indicator_raw_value] == 5),\n",
    "                (cleansed_data[indicator_raw_value] == 6),\n",
    "                (cleansed_data[indicator_raw_value] == 7)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '1.67', '3.33', '5.00', '6.67', '8.33', '10.00'] \n",
    "        \n",
    "        else:\n",
    "            raise('The scoring type you have specified does not exist. Make sure your scoring type is listed in the documentation of this fuction.') \n",
    "        \n",
    "        # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "        cleansed_data['SCALED'] = np.select(conditions, values)\n",
    "            \n",
    "    else:\n",
    "        # This is the section dealing with numerical indicators\n",
    "        # If there are dimensions in the dataset (e.g. GENDER), then the normalization of the indicator score \n",
    "        # must be done for all subgroups of all dimensions. For that, we must extract these subsets in the original dataset\n",
    "\n",
    "        # Extract the subsets (defined by the dimensions) of the data\n",
    "            # Exclude columns which are are not a dimension or should not be part in defining a subset (i.e. they aren't part of a unique identifier of a row)\n",
    "        col_list = cleansed_data.columns.to_list() # list of all columns in the dataframe\n",
    "        non_dim_cols_tuple = tuple(non_dim_cols) # parameters must be passed as list, but the following command requires a tuple\n",
    "        non_essential_col = [e for e in col_list if e not in non_dim_cols_tuple]\n",
    "        \n",
    "            # Define parameters to run a loop going through all subsets\n",
    "        length = cleansed_data[non_essential_col].drop_duplicates().shape[0]\n",
    "        width = cleansed_data[non_essential_col].drop_duplicates().shape[1]\n",
    "\n",
    "        # Create empty final dataframe before entering loop. The different subsets will be appended in this DF to obtain the full dataset incl. the scaled variable in the end\n",
    "        cleansed_data_full = pd.DataFrame(columns = cleansed_data.columns.tolist())\n",
    "        \n",
    "        # Loop: i) defining values of subsets to create ii) subsets , iii) calculate the scaled value for all of them and iv) append the subsets in one dataframe\n",
    "        for j in range(1, length):\n",
    "            # i) Create the defining values of the subset\n",
    "            subset = ''\n",
    "            for i in range(width):\n",
    "                subset += '(cleansed_data[cleansed_data[non_essential_col].columns[{}]] == cleansed_data[non_essential_col].drop_duplicates().iloc[{}, {}]) & '.format(i, j, i)\n",
    "            subset = subset.rstrip('& ')\n",
    "            \n",
    "            # ii) Define the subset of the entire dataframe \n",
    "            cleansed_data_subset = cleansed_data[eval(subset)]\n",
    "            \n",
    "            # Log: Inform user what subset the operations are being performed on\n",
    "            print('In the loop we are currently dealing with the subset #{}, which has these defining values: {}'.format(j, cleansed_data_subset[non_essential_col].drop_duplicates()))\n",
    "            print('\\n The shape of the subset is: {}'.format(cleansed_data_subset.shape)) # DELETE\n",
    "            \n",
    "            # Often, the subset will be empty dataframes, because they only consists of NaN values\n",
    "            try: \n",
    "                # iii) Determine basic descriptive statistics of the distribution that are required for the normalization\n",
    "                min_val = np.nanmin(cleansed_data_subset[indicator_raw_value].astype('float'))\n",
    "                max_val = np.nanmax(cleansed_data_subset[indicator_raw_value].astype('float'))\n",
    "                q1 =  cleansed_data_subset[indicator_raw_value].astype('float').quantile(q=0.25)\n",
    "                q2 =  cleansed_data_subset[indicator_raw_value].astype('float').quantile(q=0.50)\n",
    "                q3 =  cleansed_data_subset[indicator_raw_value].astype('float').quantile(q=0.75)                      \n",
    "                iqr = q3 - q1\n",
    "\n",
    "                # Define what max value to use for the normalization            \n",
    "                if max_val > q3 + whisker_factor * iqr:\n",
    "                    max_to_use = q3 + whisker_factor * iqr\n",
    "                    print('The distribution of the raw data values this subgroup contains outliers or is too skewed on the upper end. The maximum value to be used for the normalisation is: 3rd quartile or distribution + {} * IQR. It is: {} \\n See histogram printed below for info. \\n'.format(whisker_factor, max_to_use))\n",
    "                else:\n",
    "                    max_to_use = max_val\n",
    "                    print('The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is {}. This value corresponds to country: {} \\n'.format(max_to_use, cleansed_data[cleansed_data[indicator_raw_value].astype('float') == max_val].COUNTRY_NAME))\n",
    "\n",
    "                # Define what min value to use for the normalization\n",
    "                if min_val < q1 - whisker_factor * iqr:\n",
    "                    min_to_use = q1 - whisker_factor * iqr\n",
    "                    print('The distribution of the raw data values for this subgroup contains outliers or is too skewed on the lower end. The minimum value to be used for the normalisation is 1st quartile or distribution - {} * IQR. It is: {} \\n See histogram printed below for info. \\n'.format(whisker_factor, min_to_use))\n",
    "                else:\n",
    "                    min_to_use = min_val\n",
    "                    print('The distribution of the raw data for this subgroup does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the minimum value in the dataset, which is {}. This value corresponds to country: {} \\n'.format(min_to_use, cleansed_data[cleansed_data[indicator_raw_value].astype('float') == min_val].COUNTRY_NAME))\n",
    "\n",
    "                # If there are outliers or a skewed distribution, print the distribution for the user.\n",
    "                if (min_val < q1 - whisker_factor * iqr) or (max_val > q3 + whisker_factor * iqr):\n",
    "                    print('\\n This is the distribution of the raw data of the indicator.')\n",
    "                    print(cleansed_data_subset[indicator_raw_value].hist(bins = 30))\n",
    "\n",
    "                # Define the value range that is used for the scaling (normalization)\n",
    "                tot_range = max_val - min_val \n",
    "\n",
    "                # Compute the normalized value of the raw data in the column \"SCALED\"\n",
    "                    # Distinguish between indicators, whose value must be inverted\n",
    "                if inverted == True:\n",
    "                    cleansed_data_subset['SCALED'] = round(10 - 10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)\n",
    "                else:\n",
    "                    cleansed_data_subset['SCALED'] = round(10 * (cleansed_data_subset[indicator_raw_value].astype('float') - min_val)/ tot_range, 2)  \n",
    "\n",
    "                # iv) Append the subset including its scaled value to the final returned dataframe\n",
    "                    # Right join to have all countries from the final crba master list\n",
    "                cleansed_data_subset_rj = cleansed_data_subset.merge(\n",
    "                       right = crba_final_country_list,\n",
    "                      how = 'right',\n",
    "                      left_on = cleansed_df_iso2_col,\n",
    "                      right_on = crba_final_country_list_iso_col,\n",
    "                      indicator = 'RJ_CRBA_FULL_LIST'\n",
    "                  )\n",
    "\n",
    "                    # Append the values\n",
    "                cleansed_data_full = cleansed_data_full.append(cleansed_data_subset_rj)\n",
    "\n",
    "            except:\n",
    "                print('Dataframe is empty. There are no values to append.')\n",
    "                    \n",
    "    # Bring the final dataframe with scaled (normalized) values from wide to long format\n",
    "        # Prepare the melting of the dataframe, by defining what columns remain untouched by the melt\n",
    "    kept_columns = [x for x in cleansed_data_full.columns.tolist() if x not in [indicator_raw_value, 'SCALED']]\n",
    "    \n",
    "        # Bring the dataframe from wide to long format and return it\n",
    "    return(\n",
    "        pd.melt(cleansed_data_full,\n",
    "           id_vars = kept_columns,\n",
    "           value_vars = [indicator_raw_value, 'SCALED'],\n",
    "           var_name = 'VALUE_TYPE')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #DEVELOPMENT; DELETE AFTER\n",
    "    # save_cleansed_data(dataframe = cleansed_data, # DELETE\n",
    "    #         filename = 'dev-cleansed.xlsx.xlsx') # DELETE\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s55_cleansed[eval(subset)]['OBS_VALUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude columns which are value or should not be part of the composite key (i.e. the unique identifier of a row)\n",
    "col_list = s55_cleansed.columns.to_list() # list of all columns in the dataframe\n",
    "non_essential_col = [e for e in col_list if e not in ('TIME_PERIOD', 'REF_AREA', 'OBS_VALUE', 'COUNTRY_ISO_3', 'COUNTRY_NAME', 'COUNTRY_ISO_2')] \n",
    "\n",
    "# Perpare loop to identify the distint subsets in the datasets\n",
    "length = s55_cleansed[non_essential_col].drop_duplicates().shape[0]\n",
    "width = s55_cleansed[non_essential_col].drop_duplicates().shape[1]\n",
    "\n",
    "# Find the different subsets and their column values\n",
    "# Loop to identify the \"length\" number of subsets:\n",
    "for j in range(length):\n",
    "    # Loop to create the column values defining this subgroup\n",
    "    subset = ''\n",
    "    for i in range(width):\n",
    "        subset += '(s55_cleansed[s55_cleansed[non_essential_col].columns[{}]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[{}, {}]) & '.format(i, j, i)\n",
    "    subset = subset.rstrip('& ')\n",
    "    \n",
    "    # This is where the remaining code should start\n",
    "    print(s55_cleansed[eval(subset)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s55_cleansed = s55_cleansed.assign()\n",
    "\n",
    "# s55_cleansed.loc[s55_cleansed[eval(subset)], 'SCALED'] = 10 \n",
    "\n",
    "# s55_cleansed_subset_1 = s55_cleansed[some_condition.values]\n",
    "\n",
    "#s55_cleansed[some_condition.values]['SCALED'] = 10\n",
    "\n",
    "#s55_cleansed.reindex(columns = ['SCALED'])   \n",
    "\n",
    "# s55_cleansed.assign('SCALED' == )\n",
    "\n",
    "# s55_cleansed[eval(subset)]['SCALED']\n",
    "# s55_cleansed[s55_cleansed[some_condition]['SCALED'] = 10]\n",
    "# s55_cleansed.assign['SCALED' = ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            # Distinguish between indicators, whose value must be inverted\n",
    "            if inverted == True:\n",
    "                temp = cleansed_data.assign(SCALED = round(10 - 10 * (cleansed_data[indicator_raw_value].astype('float') - min_val)/ tot_range, 2))\n",
    "            else:\n",
    "                temp = cleansed_data.assign(SCALED = round(10 * (cleansed_data[indicator_raw_value].astype('float') - min_val)/ tot_range, 2))\n",
    "\n",
    "                \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        # Identify if there are subgroups along the dimensions\n",
    "            # Exclude columns year, country and value\n",
    "        col_list = cleansed_data.columns.to_list() # list of all columns in the dataframe\n",
    "        non_essential_col = [e for e in col_list if e not in (country_col, year_col, indicator_raw_value)] \n",
    "\n",
    "            # Take remaining columns to find out if there are subgroups\n",
    "        length = cleansed_data[non_essential_col].drop_duplicates().shape[0]\n",
    "        width = cleansed_data[non_essential_col].drop_duplicates().shape[1]\n",
    "\n",
    "        # Extract the different subsets in the dataset and do the normalization for each one of them\n",
    "        for j in range(length):\n",
    "            subset = ''\n",
    "            for i in range(width):\n",
    "                subset += '(s55_cleansed[s55_cleansed[non_essential_col].columns[{}]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[{}, {}]) & '.format(i, j, i)\n",
    "                subset = subset.rstrip('& ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s55_cleansed[non_essential_col].drop_duplicates().iloc[0, 22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s55_cleansed[non_essential_col][\n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[0]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 0]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[1]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 1]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[2]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 2]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[3]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 3]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[4]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 4]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[5]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 5]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[6]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 6]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[7]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 7]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[8]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 8]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[9]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 9]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[10]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 10]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[11]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 11]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[12]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 12]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[13]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 13]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[14]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 14]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[15]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 15]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[16]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 16]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[17]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 17]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[18]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 18]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[19]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 19]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[20]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 20]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[21]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 21]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[22]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 22]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[23]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 23]) &\n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[5]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 5])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = s55_cleansed.columns.to_list() # list of all columns in the dataframe\n",
    "non_essential_col = [e for e in col_list if e not in ('TIME_PERIOD', 'REF_AREA', 'OBS_VALUE', 'COUNTRY_ISO_3', 'COUNTRY_NAME', 'COUNTRY_ISO_2')] \n",
    "\n",
    "s55_cleansed[non_essential_col].drop_duplicates().shape[1]\n",
    "\n",
    "length = s55_cleansed[non_essential_col].drop_duplicates().shape[0]\n",
    "width = s55_cleansed[non_essential_col].drop_duplicates().shape[1]\n",
    "\n",
    "s55_cleansed['STA_UNIT'] = s55_cleansed['STAT_UNIT'].astype(str)\n",
    "\n",
    "condition = \"s55_cleansed['Dataflow'] == 'UNESCO:SDG4(2.0)'\"\n",
    "\n",
    "condiction_rebuilt = \"s55_cleansed[non_essential_col].columns[0] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 0]\"\n",
    "\n",
    "print(s55_cleansed[non_essential_col].columns[0])\n",
    "\n",
    "# str(s55_cleansed[non_essential_col].columns[0])\n",
    "\n",
    " #s55_cleansed.str(s55_cleansed[non_essential_col].columns[0])\n",
    "\n",
    "# s55_cleansed[s55_cleansed.str(s55_cleansed[non_essential_col].columns[0]) == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 0]]\n",
    "\n",
    "\n",
    "#eval(condition)\n",
    "\n",
    "s55_cleansed[eval(condition)]\n",
    "\n",
    "# s55_cleansed[s55_cleansed[non_essential_col].columns[0] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1]\n",
    "\n",
    "# s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 0]\n",
    "\n",
    "s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 1]\n",
    "\n",
    "\n",
    "'''\n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[0]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 0]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[1]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 1]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[2]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 2]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[3]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 3]) & \n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[4]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 4]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[5]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 5]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[6]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 6]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[7]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 7]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[8]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 8]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[9]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 9]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[10]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 10]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[11]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 11]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[12]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 12]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[13]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 13]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[14]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 14]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[15]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 15]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[16]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 16]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[17]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 17]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[18]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 18]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[19]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 19]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[20]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 20]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[21]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 21]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[22]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 22]) & (s55_cleansed[s55_cleansed[non_essential_col].columns[23]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 23]) &\n",
    "(s55_cleansed[s55_cleansed[non_essential_col].columns[24]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, 24])\n",
    "'''\n",
    "\n",
    "\n",
    "# Find the different subsets and their column values\n",
    "# Loop to identify the \"length\" number of subsets:\n",
    "# for j in range(length):\n",
    "\n",
    "# Loop to create the column values defining this subgroup\n",
    "subset = ''\n",
    "for i in range(width):\n",
    "    subset += '(s55_cleansed[s55_cleansed[non_essential_col].columns[{}]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, {}]) & '.format(i, i)\n",
    "subset = subset.rstrip('& ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [: width]:\n",
    " #   subset = ''\n",
    " #   subset += '(s55_cleansed[s55_cleansed[non_essential_col].columns[{}]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[1, {}]'.format(i, i)\n",
    "  #  return(subset)\n",
    "    \n",
    "s55_cleansed[\n",
    "    (s55_cleansed[s55_cleansed[non_essential_col].columns[0]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[2, 0]) & \n",
    "    (s55_cleansed[s55_cleansed[non_essential_col].columns[1]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[2, 1]) & \n",
    "    (s55_cleansed[s55_cleansed[non_essential_col].columns[2]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[2, 2]) & \n",
    "    (s55_cleansed[s55_cleansed[non_essential_col].columns[3]] == s55_cleansed[non_essential_col].drop_duplicates().iloc[2, 3])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Discard rows of countries that are not in the master country list\n",
    "        # The raw data only contains country names. Assign ISO codes to these country names\n",
    "    # grouped_data_iso = grouped_data.merge(\n",
    "    #     right = country_list_full,\n",
    "    #    how = 'left',\n",
    "    #    left_on = 'REF_AREA',\n",
    "    #    right_on = country_list_full_name_col,\n",
    "    #    validate = 'many_to_one')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# cleanse raw data\n",
    "s55_cleansed = cleanse_unesco_api_data(\n",
    "    uis_data = s55_raw,\n",
    "    country_df = country_crba_list,\n",
    "    country_df_iso2_col = 'COUNTRY_ISO_2',\n",
    "    columns = ['EDU_CAT', 'SEX', 'AGE'],\n",
    "    most_recent_only = True)\n",
    "\n",
    "# save cleansed data\n",
    "save_cleansed_data(dataframe = s55_cleansed,\n",
    "             filename = 'S_55_cleansed.xlsx')\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanse_sdg_api_data(raw_data):\n",
    "    \"\"\"Transform raw data from sdg API source to distil relevant data \n",
    "\n",
    "    Discard columns and aggregate data to retrieve the latest value for all countries in a given dataframe for a given SDG API series.\n",
    "\n",
    "    Parameters:\n",
    "    raw_data (obj): Return of function 'extract_sdg_api_data'. Should be a pandas dataframe \n",
    "    \n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe\n",
    "\n",
    "   \"\"\"\n",
    "    # Extract relevant columns\n",
    "    stage_data = raw_data[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "    \n",
    "    # Get the latest value for each country of the series\n",
    "    return(stage_data[stage_data['timePeriodStart'] == stage_data.groupby('geoAreaName')['timePeriodStart'].transform('max')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw data contains data on BOTHSEX, MALE AND FEMALE, limit ourseleves to BOTHSEXES\n",
    "temp = s24_raw[s24_raw['dimensions.Sex'] == 'BOTHSEX']\n",
    "\n",
    "# Transform the raw data\n",
    "s24_cleansed = cleanse_sdg_api_data(raw_data = temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s24_raw[s24_raw['geoAreaName'] == 'Burkina Faso']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #\n",
    "    #\n",
    "    #\n",
    "    '''\n",
    "    # Discard rows of countries that are not in the master country list\n",
    "        # The raw data only contains country names. Assign ISO codes to these country names    \n",
    "    raw_data_iso = raw_data.merge(\n",
    "        right = country_list_full,\n",
    "        how = 'left',\n",
    "        left_on = 'geoAreaName',\n",
    "        right_on = country_list_full_name_col,\n",
    "        validate = 'many_to_one')\n",
    "    \n",
    "        # Discard countries that aren't part of the final CRBA master list\n",
    "    contry_filt_data = raw_data_iso.merge(\n",
    "        right = country_df,\n",
    "        how = 'right', \n",
    "        left_on = country_list_full_iso2_col,\n",
    "        right_on = country_df_iso2_col,\n",
    "        indicator = True,\n",
    "        validate = 'many_to_one')\n",
    "         \n",
    "    # Get the latest value for each country of the series\n",
    "    col_list = contry_filt_data.columns.to_list() # list of all columns in the dataframe\n",
    "    col_list_gb = [e for e in col_list if e not in ('value', 'timePeriodStart')] # exclude timePeriodStart and value, because these one'saren't used for the groupby statement\n",
    "    \n",
    "    # Some of the columns contain values which Python interprets as lists (e.g. [8.1]). These make the groupby statement malfunction. Convert them to string\n",
    "    contry_filt_data[col_list_gb] =  contry_filt_data[col_list_gb].astype(str)\n",
    "    \n",
    "    # Retreive the latest available data for each group, where group is 'col_list_gb'\n",
    "    latest_year_data = contry_filt_data[contry_filt_data['timePeriodStart'] == contry_filt_data.groupby(col_list_gb)['timePeriodStart'].transform('max')]\n",
    "    \n",
    "    # return result\n",
    "    return(latest_year_data.sort_values(by = country_df_iso2_col, axis = 0))\n",
    "    ''' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(latest_year_data.COUNTRY_ISO_2.describe())\n",
    "    print(contry_filt_data.COUNTRY_ISO_2.describe())\n",
    "    \n",
    "# Include a confirmation whether all of this went well\n",
    "    print(raw_data.shape)\n",
    "    print(country_df.shape)\n",
    "    print(raw_data_iso.shape)\n",
    "    print(contry_filt_data.shape)\n",
    "    print(contry_filt_data[contry_filt_data._merge == 'left_only'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s24_raw.geoAreaName.describe()\n",
    "country_full_list.CountryDesc.describe()\n",
    "\n",
    "col_list = s24_raw.columns.to_list() # list of all columns in the dataframe\n",
    "print(col_list)\n",
    "col_list_gb = [e for e in col_list if e not in ('value', 'timePeriodStart')]\n",
    "print(col_list_gb)\n",
    "\n",
    "# s24_raw['timePeriodStart'] = \n",
    "\n",
    "# s24_raw.groupby(by = 'geoAreaName')['timePeriodStart'].max()\n",
    "\n",
    "s24_raw[col_list_gb] = s24_raw[col_list_gb].astype(str)\n",
    "\n",
    "s24_raw[s24_raw['timePeriodStart'] == s24_raw.groupby(col_list_gb)['timePeriodStart'].transform('max')]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    ''' No this codeblock will probably be discarded\n",
    "    # Extract relevant columns\n",
    "    stage_data = raw_data[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "    '''\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "THIS IS ONLY ABOUT EXTRACTING THE LATEST VALUE AND ABOUT FILTERING OUT COUNTRIES WHO SHOULDN'T BE IN THERE\n",
    "\n",
    "\n",
    "-----\n",
    "\n",
    "\n",
    "    # Discard unnecessay columns\n",
    "    columns_kept = ['REF_AREA', 'TIME_PERIOD','OBS_VALUE'] + columns\n",
    "    uis_data = uis_data[columns_kept]\n",
    "    \n",
    "    if most_recent_only == True:\n",
    "        # Retrieve the most up-to-date number for each country\n",
    "        uis_data = uis_data[uis_data['TIME_PERIOD']==uis_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    # Discard rows of countries that are not in the master country list\n",
    "    cleansed_final = uis_data.merge(\n",
    "        right = country_df,\n",
    "        how = 'right', \n",
    "        left_on = 'REF_AREA',\n",
    "        right_on = country_df_iso2_col,\n",
    "        validate = 'one_to_one').assign(indicator = 'I-15')\n",
    "    \n",
    "    return(cleansed_final.sort_values(by = country_df_iso2_col, axis = 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate data to the\n",
    "s56_final = country_iso_list.merge(right = s56_scaled,\n",
    "                 how = 'left',\n",
    "                 left_on = 'CountryIso2',\n",
    "                 right_on = 'REF_AREA',\n",
    "                 validate = 'many_to_one').assign(indicator ='I-16')\n",
    "\n",
    "# Check if some countries got lost during the join\n",
    "print('The number of rows in the raw data set was {}. The number of available datapoints in the final dataframe is {}. Therefore, {} rows got lost during the join.'.format(\n",
    "    len(s56_scaled), \n",
    "    s56_final.OBS_VALUE.notna().sum(), \n",
    "    len(s56_scaled) - s56_final.OBS_VALUE.notna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left join to counry list to bring it into target format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left join data to the target format\n",
    "s55_final = country_iso_list.merge(right = s55_scaled,\n",
    "                 how = 'left',\n",
    "                 left_on = 'CountryIso2',\n",
    "                 right_on = 'REF_AREA',\n",
    "                 validate = 'one_to_many').assign(indicator = 'I-15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_raw_data(dataframe, filename, output_path = data_sources_raw):\n",
    "    \"\"\"Save raw data \n",
    "\n",
    "    Parameters:\n",
    "    dataframe (obj): Pandas dataframe to be stored\n",
    "    filename (string): The way you would like o anem your file. Must include the extension (for example .xlsx)\n",
    "    output_path (string): Folder where data is stored\n",
    "\n",
    "    Returns:\n",
    "    obj: \n",
    "\n",
    "   \"\"\"\n",
    "    dataframe.to_excel(output_path + filename)\n",
    "    print('The raw data has been saved as .xlsx file in: ' + output_path)\n",
    "\n",
    "    \n",
    "def scaler(raw_dataframe, indicator_raw_value, cat_var = False, cat_scoring_type = None, inverted = False, whisker_factor = 1.5):\n",
    "    \"\"\" Transform raw data into scaled (normalized) data in long format\n",
    "    \n",
    "    Parameters:\n",
    "    raw_dataframe (obj): Raw dataset, pandas dataframe\n",
    "    indicator_raw_value (str): Column containing the actual raw data values \n",
    "    cat_var (bool): is variable of type categorical or numerical?\n",
    "    cat_scoring_type (str): For categorical variables, specify the scoring type (i.e. what values the categorical variable can take)\n",
    "    inverted (bool): If we are dealing with a numerical variable, should the values be inverted?\n",
    "    whisker_factor (float): \n",
    "\n",
    "    Returns:\n",
    "    obj: Returns numpy array with transformed values\n",
    "    \"\"\"\n",
    "    # Check if an indicator is numeric of categorical\n",
    "    if cat_var == True:\n",
    "        # This is the section dealing with categorical indicators\n",
    "        # There are various scoring_types a categorical variable can take. For each one, different labels apply to the values\n",
    "        # Type 2-1-0\n",
    "        if cat_scoring_type == 'Type 2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0),\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '10.00']\n",
    "\n",
    "        # Type 2-1\n",
    "        elif cat_scoring_type == 'Type 2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '10.00']\n",
    "\n",
    "        # Type 3-2-1\n",
    "        elif cat_scoring_type == 'Type 3-2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '5.00', '10.00']\n",
    "\n",
    "        # Type 3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0)\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '5.00', '10.00']            \n",
    "            \n",
    "        # Type 4-3-2-1\n",
    "        elif cat_scoring_type == 'Type 4-3-2-1':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3),\n",
    "                (raw_dataframe[indicator_raw_value] == 4)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['0.00', '3.33', '6.67', '10.00']                \n",
    "            \n",
    "            \n",
    "            # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "            raw_dataframe['SCALED'] = np.select(conditions, values)\n",
    "\n",
    "        # Type 4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0)\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3),\n",
    "                (raw_dataframe[indicator_raw_value] == 4)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '3.33', '6.67', '10.00'] \n",
    "\n",
    "\n",
    "        # Type 5-4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 5-4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0)\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3),\n",
    "                (raw_dataframe[indicator_raw_value] == 4),\n",
    "                (raw_dataframe[indicator_raw_value] == 5)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '2.50', '5.00', '7.50', '10.00']\n",
    "            \n",
    "        # Type 7-6-5-4-3-2-1-0\n",
    "        elif cat_scoring_type == 'Type 7-6-5-4-3-2-1-0':\n",
    "            # Specify the values the raw data can take\n",
    "            conditions = [\n",
    "                (raw_dataframe[indicator_raw_value] == 0)\n",
    "                (raw_dataframe[indicator_raw_value] == 1),\n",
    "                (raw_dataframe[indicator_raw_value] == 2),\n",
    "                (raw_dataframe[indicator_raw_value] == 3),\n",
    "                (raw_dataframe[indicator_raw_value] == 4),\n",
    "                (raw_dataframe[indicator_raw_value] == 5),\n",
    "                (raw_dataframe[indicator_raw_value] == 6),\n",
    "                (raw_dataframe[indicator_raw_value] == 7)\n",
    "                ]\n",
    "            \n",
    "            # Specify the normalized values\n",
    "            norm_values = ['No data', '0.00', '1.67', '3.33', '5.00', '6.67', '8.33', '10.00'] \n",
    "        \n",
    "        else:\n",
    "            raise('The scoring type you have specified does not exist. Make sure your scoring type is listed in the documentation of this fuction.') \n",
    "        \n",
    "        # create a new column and use np.select to assign values to it using our lists as arguments\n",
    "        raw_dataframe['SCALED'] = np.select(conditions, values)\n",
    "            \n",
    "    else:\n",
    "        # This is the section dealing with numerical indicators\n",
    "\n",
    "        # Determine basic descriptive statistics of the distribution\n",
    "        min_val = min(raw_dataframe[indicator_raw_value].astype('float'))\n",
    "        max_val = max(raw_dataframe[indicator_raw_value].astype('float'))\n",
    "        q1 =  raw_dataframe[indicator_raw_value].astype('float').quantile(q=0.25)\n",
    "        q2 =  raw_dataframe[indicator_raw_value].astype('float').quantile(q=0.50)\n",
    "        q3 =  raw_dataframe[indicator_raw_value].astype('float').quantile(q=0.75)                      \n",
    "        iqr = q3 - q1\n",
    "        \n",
    "        # Define what max value to use for the normalization            \n",
    "        if max_val > q3 + whisker_factor * iqr:\n",
    "            max_to_use = q3 + whisker_factor * iqr\n",
    "            print('The distribution of the raw data values contains outliers or is too skewed on the upper end. The maximum value to be used for the normalisation is the the 3rd quartile added to IQR multiplied by the {}. It is: {} \\n See histogram printed below for info. \\n'.format(whisker_factor, max_to_use))\n",
    "        else:\n",
    "            max_to_use = max_val\n",
    "            print('The distribution of the raw data does not contain outliers on the upper end. The maximum value used for the normalisation is the maximum value in the dataset, which is {}. \\n'.format(max_to_use))\n",
    "                      \n",
    "        # Define what min value to use for the normalization\n",
    "        if min_val < q1 - whisker_factor * iqr:\n",
    "            min_to_use = q1 - whisker_factor * iqr\n",
    "            print('The distribution of the raw data values contains outliers or is too skewed on the lower end. The minimum value to be used for the normalisation is the the 3rd quartile added to IQR multiplied by the the {}. It is: {} \\n See histogram printed below for info. \\n'.format(whisker_factor, min_to_use))\n",
    "        else:\n",
    "            min_to_use = min_val\n",
    "            print('The distribution of the raw data does not contain outliers or is too skewed on the lower end. The minimum value used for the normalisation is the maximum value in the dataset, which is {}. \\n'.format(min_to_use))\n",
    "        \n",
    "        # If there are outliers or a skewed distribution, print the distribution for the user.\n",
    "        if (min_val < q1 - whisker_factor * iqr) or (max_val > q3 + whisker_factor * iqr):\n",
    "            print('\\n This is the distribution of the raw data of the indicator.')\n",
    "            print(raw_dataframe[indicator_raw_value].hist(bins = 30))\n",
    "                       \n",
    "        # Define the value range that is used for the scaling (normalization)\n",
    "        tot_range = max_val - min_val \n",
    "        \n",
    "        # Distinguish between indicators, whose value must be inverted\n",
    "        if inverted == True:\n",
    "            temp = raw_dataframe.assign(SCALED = round(10 - 10 * (raw_dataframe[indicator_raw_value].astype('float') - min_val)/ tot_range, 2))\n",
    "        else:\n",
    "            temp = raw_dataframe.assign(SCALED = round(10 * (raw_dataframe[indicator_raw_value].astype('float') - min_val)/ tot_range, 2))\n",
    "        \n",
    "        # bring dataframe from wide to long format\n",
    "        # Prepare the melting of the dataframe, by defining what columns remain untouched by the melt\n",
    "    kept_columns = [x for x in raw_dataframe.columns.tolist() if x not in [indicator_raw_value, 'SCALED']]\n",
    "\n",
    "    return(\n",
    "        pd.melt(temp,\n",
    "           id_vars = kept_columns,\n",
    "           value_vars = [indicator_raw_value, 'SCALED'],\n",
    "           var_name = 'VALUE_TYPE')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNESCO API specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_unesco_api_data(stat_unit, unit, edu_level, subs_key, start_period = 2010, end_period = 2020):\n",
    "    \"\"\"Extract raw data from the UNESCO API\n",
    "\n",
    "    The API can be called with a API key, which you must generate by creating an account. \n",
    "    The calls can be done with simple https requests, in which many parameter must be specified (e.g. indicator code, sexes, age, ...). TO understand the structure, please check the query builder https://apiportal.uis.unesco.org/query-builder\n",
    "\n",
    "    Parameters:\n",
    "    stat_unit (string): Indicatiro code, I suggest you check the query builder https://apiportal.uis.unesco.org/query-builder to retrieve the right code\n",
    "    unit (string): Specify the unnit of measure, e.g. 'PT' for percentage total\n",
    "    edu_level (string): Choose educational level, L2 = lower secondary, L3 = upper secondary age\n",
    "    subs_key (string): Your API key. You must create one. Visit https://apiportal.uis.unesco.org/getting-started for more info\n",
    "    start_period (int): Year which is the beginning of series for which you want to collect data\n",
    "    end_period (ing): Year which is the end of series for which you want to collect data\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe\n",
    "\n",
    "   \"\"\"\n",
    "    url = 'https://api.uis.unesco.org/sdmx/data/SDG4/{sunit}.{umeasure}.{elevel}._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(\n",
    "        sunit = stat_unit,\n",
    "        umeasure = unit,\n",
    "        elevel = edu_level,\n",
    "        syear = start_period,\n",
    "        eyear = end_period,\n",
    "        skey = subs_key)\n",
    "    return(pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',))\n",
    "\n",
    "def cleanse_unesco_api_data(uis_data, country_iso2_list, columns, most_recent_only = True):\n",
    "    \"\"\"< To do >\n",
    "\n",
    "    Parameters:\n",
    "    s15_raw (obj): Should be return of function s55_extract or s56_extract\n",
    "    country_iso2_list (array): A numpy array of 2-character country codes.\n",
    "    columns: List of the dimensions (i.e. columns) that are you want to keep for the indicator beyond the main necessary ones\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe with the relevant data of the indicator\n",
    "\n",
    "   \"\"\"\n",
    "    # Discard unnecessay columns\n",
    "    columns_kept = ['REF_AREA', 'TIME_PERIOD','OBS_VALUE'] + columns\n",
    "    uis_data = uis_data[columns_kept]\n",
    "    \n",
    "    # Discard rows of countries that are in the master country list\n",
    "    uis_data = uis_data[uis_data.REF_AREA.isin(country_iso2_list)]\n",
    "    \n",
    "    if most_recent_only == True:\n",
    "        # Retrieve the most up-to-date number for each country\n",
    "        uis_data = uis_data[uis_data['TIME_PERIOD']==uis_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return(uis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDG API specific functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sdg_api_data(series_code):\n",
    "    \"\"\"Extract raw data for a series \n",
    "\n",
    "    Extract data on the specifies series as json and flatten it out into a pandas dataframe\n",
    "    To retrieve the data from the API of the SDG indicators, you must proceed as follows: \n",
    "    \n",
    "    * Find the seriesCode of the indicator for which you want to retrieve data: I recommend visiting https://unstats.un.org/sdgs/indicators/database/ and browse the indicator you want and the series code will be indicated there\n",
    "    * Visit https://unstats.un.org/SDGAPI/swagger/#!/Series/V1SdgSeriesDataGet\n",
    "    * Expand the tab GET /v1/sdg/Series/Data\n",
    "    * Type in the seriesCode, this will provide you with the link to the JSON\n",
    "    * Important: The results will be on shwon on various pages if the dataset is too large. That is why it is a good idea to set a pageSize value large enough to accomodate all data in just one page to not have to iterate over various pages\n",
    "\n",
    "        \n",
    "    Parameters:\n",
    "    series (string): Code of the series, which can be found by browsing for the relevant series here:https://unstats.un.org/sdgs/indicators/database?indicator=16.2.2\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe\n",
    "\n",
    "   \"\"\"\n",
    "    url = 'https://unstats.un.org/SDGAPI/v1/sdg/Series/Data?seriesCode={}&pageSize=999999999'.format(series_code)\n",
    "    return(pd.json_normalize(requests.get(url).json()['data']))    \n",
    "\n",
    "\n",
    "def cleanse_sdg_api_data(raw_data):\n",
    "    \"\"\"Transform raw data from sdg API source to distil relevant data \n",
    "\n",
    "    Discard columns and aggregate data to retrieve the latest value for all countries in a given dataframe for a given SDG API series.\n",
    "\n",
    "    Parameters:\n",
    "    raw_data (obj): Return of function 'extract_sdg_api_data'. Should be a pandas dataframe \n",
    "    \n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe\n",
    "\n",
    "   \"\"\"\n",
    "    # Extract relevant columns\n",
    "    stage_data = raw_data[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "    # Get the latest value for each country of the series\n",
    "    return(stage_data[stage_data['timePeriodStart'] == stage_data.groupby('geoAreaName')['timePeriodStart'].transform('max')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def scaler(raw_dataframe, indicator_value, cat_var = False, inverted = False, up_lim = None, low_lim = None):\n",
    "    \"\"\" Transform raw data into scaled (normalized) data in long format\n",
    "    \n",
    "    Parameters:\n",
    "    raw_dataframe (obj): Raw dataset, pandas dataframe\n",
    "    indicator_value (str): Column containing the actual raw data values \n",
    "    cat_var (bool): is variable of type categorical or numerical?\n",
    "    inverted (bool): If we are dealing with a numerical variable, should the values be inverted?\n",
    "    up_lim_req (float): For numerical variables, is there an artificial upper limit \n",
    "    low_lim_req (bool): For numerical variables, is there an artificial upper limit\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns numpy array with transformed values\n",
    "    \"\"\"\n",
    "    # Check if an indicator is numeric of categorical\n",
    "    if cat_var == True:\n",
    "        pass\n",
    "    else:\n",
    "        # This is the section dealing with numerical indicators\n",
    "        # Define the range of the indicator values (some take their max/min value, sometimes an upper/lower limit)\n",
    "        if up_lim == None:\n",
    "            max_val = max(raw_dataframe[indicator_value].astype('float'))\n",
    "        else:\n",
    "            max_val = up_lim\n",
    "        if low_lim == None:\n",
    "            min_val = min(raw_dataframe[indicator_value].astype('float'))\n",
    "        else:\n",
    "            min_val = low_lim \n",
    "        \n",
    "        # Define the value range that is used for the scaling (normalization)\n",
    "        tot_range = max_val - min_val \n",
    "        \n",
    "        # Distinguish between indicators, whose value must be inverted\n",
    "        if inverted == True:\n",
    "            temp = raw_dataframe.assign(SCALED = round(10 - 10 * (raw_dataframe[indicator_value].astype('float') - min_val)/ tot_range, 2))\n",
    "        else:\n",
    "            temp = raw_dataframe.assign(SCALED = round(10 * (raw_dataframe[indicator_value].astype('float') - min_val)/ tot_range, 2))\n",
    "        \n",
    "        # bring dataframe from wide to long format\n",
    "        # Prepare the melting of the dataframe, by defining what columns remain\n",
    "        kept_columns = [x for x in s55_cleansed.columns.tolist() if x not in [indicator_value, 'SCALED']]\n",
    "        \n",
    "        return(\n",
    "            pd.melt(temp,\n",
    "               id_vars = ['REF_AREA', 'TIME_PERIOD', 'EDU_CAT', 'SEX'],\n",
    "               value_vars = [indicator_value, 'SCALED'],\n",
    "               var_name = 'VALUE_TYPE')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(s55_final) == len(s55_scaled) + len(country_iso_list) - len(s55_scaled)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(indicator, cat_var = False, inverted = False, up_lim = None, low_lim = None):\n",
    "    \"\"\" Transform raw data into scaled (normalized) data in long format\n",
    "    \n",
    "    Parameters:\n",
    "    indicator (obj): numpy array containing the indicator data\n",
    "    cat_var (bool): is variable of type categorical or numerical?\n",
    "    inverted (bool): If we are dealing with a numerical variable, should the values be inverted?\n",
    "    up_lim_req (float): For numerical variables, is there an artificial upper limit \n",
    "    low_lim_req (bool): For numerical variables, is there an artificial upper limit\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns numpy array with transformed values\n",
    "    \"\"\"\n",
    "    # Check if an indicator is numeric of categorical\n",
    "    if cat_var == True:\n",
    "        pass\n",
    "    else:\n",
    "        # This is the section dealing with numerical indicators\n",
    "        # Define the range of the indicator values (some take their max/min value, sometimes an upper/lower limit)\n",
    "        if up_lim == None:\n",
    "            max_val = max(indicator.astype('float'))\n",
    "        else:\n",
    "            max_val = up_lim\n",
    "        if low_lim == None:\n",
    "            min_val = min(indicator.astype('float'))\n",
    "        else:\n",
    "            min_val = low_lim \n",
    "        \n",
    "        # Define the value range that is used for the scaling (normalization)\n",
    "        tot_range = max_val - min_val \n",
    "        \n",
    "        # Distinguish between indicators, whose value must be inverted\n",
    "        if inverted == False:\n",
    "            return(round(10 * (indicator.astype('float') - min_val)/ tot_range, 2))\n",
    "        else:\n",
    "            return(round(10 - 10 * (indicator.astype('float') - min_val)/ tot_range, 2))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_unesco_api_data(uis_data, country_code_list):\n",
    "    \"\"\"< To do >\n",
    "\n",
    "    Parameters:\n",
    "    s15_raw (obj): Should be return of function s55_extract or s56_extract\n",
    "    country_code_list (array): A numpy array of 2-character country codes.  \n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe with the relevant data of the indicator\n",
    "\n",
    "   \"\"\"\n",
    "    # Discard unnecessay columns\n",
    "    uis_data = uis_data[['REF_AREA',\n",
    "         'TIME_PERIOD',\n",
    "         'OBS_VALUE']]\n",
    "    \n",
    "    # Discard unnecessary rows\n",
    "    uis_data = uis_data[uis_data.REF_AREA.isin(country_code_list)]\n",
    "    \n",
    "    # Retrieve the most up-to-date number for each country\n",
    "    uis_data = uis_data[uis_data['TIME_PERIOD']==uis_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]\n",
    "    \n",
    "    return(uis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to create the country list (which has then been manually modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list which contains all variations of names a country can have. \n",
    "# NB: CountryDesc is the key, so there are duplicate ISO codes in this list \n",
    "\n",
    "# This is the list of James, which already contains the \"normal\" and the \"long\" name of countries\n",
    "daniele_list_1 = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['Country', 'Alpha2', 'Alpha3'])\n",
    "daniele_list_1.columns = ['CountryDesc', 'CountryIso2', 'CountryIso3']\n",
    "\n",
    "# This is the list of Daniele \n",
    "daniele_list_2 = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['CountryDesc', 'CountryIso2', 'CountryIso3'])\n",
    "\n",
    "# This is the list of me\n",
    "my_list = pd.read_excel(\n",
    "    io = \"D:/Documents/2020/28_UNICEF/9_misc/CountryMaster_v3.xlsx\", \n",
    "    )[[#'CountryID',\n",
    "   'CountryDesc',\n",
    "   'CountryIso2',\n",
    "   'CountryIso3']]\n",
    "\n",
    "# Create list with all possible variations of country names\n",
    "country_full_list = daniele_list_1.append(daniele_list_2).append(my_list).drop_duplicates()\n",
    "    \n",
    "# Create list which has countryIso as unique key\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'CountryIso2')\n",
    "\n",
    "# Save files \n",
    "country_full_list.to_excel(cwd + 'all_countrynames_list.xlsx')\n",
    "country_full_list.to_csv(cwd + 'all_countrynames_list.csv',\n",
    "                        sep = \";\") # there were a few empty fields for ISO2 and ISO3 which I filled manually. \n",
    "# Furthermore I have deleted the following from the dataset: Channel Islands, Serbia and Montenegro, United nations, Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the list which contains all variations of names a country can have. \n",
    "# NB: CountryDesc is the key, so there are duplicate ISO codes in this list \n",
    "\n",
    "# This is the list of James, which already contains the \"normal\" and the \"long\" name of countries\n",
    "daniele_list_1 = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['Country', 'Alpha2', 'Alpha3'])\n",
    "daniele_list_1.columns = ['CountryDesc', 'CountryIso2', 'CountryIso3']\n",
    "\n",
    "# This is the list of Daniele \n",
    "daniele_list_2 = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['CountryDesc', 'CountryIso2', 'CountryIso3'])\n",
    "\n",
    "# This is the list of me\n",
    "my_list = pd.read_excel(\n",
    "    io = \"D:/Documents/2020/28_UNICEF/9_misc/CountryMaster_v3.xlsx\", \n",
    "    )[[#'CountryID',\n",
    "   'CountryDesc',\n",
    "   'CountryIso2',\n",
    "   'CountryIso3']]\n",
    "\n",
    "# Create list with all possible variations of country names\n",
    "country_full_list = daniele_list_1.append(daniele_list_2).append(my_list).drop_duplicates()\n",
    "    \n",
    "# Create list which has countryIso as unique key\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'CountryIso2')\n",
    "\n",
    "# Save files \n",
    "country_full_list.to_excel(cwd + 'all_countrynames_list.xlsx')\n",
    "country_full_list.to_csv(cwd + 'all_countrynames_list.csv',\n",
    "                        sep = \";\") # there were a few empty fields for ISO2 and ISO3 which I filled manually. \n",
    "# Furthermore I have deleted the following from the dataset: Channel Islands, Serbia and Montenegro, United nations, Unknown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "country_full_list = pd.read_excel(\n",
    "    io = \"D:/Documents/2020/28_UNICEF/9_misc/CountryMaster_v3.xlsx\", \n",
    "    )[[#'CountryID',\n",
    "   'CountryDesc',\n",
    "   'CountryIso2',\n",
    "   'CountryIso3']].drop_duplicates()\n",
    "\n",
    "country_iso_list = country_full_list.drop_duplicates(subset = 'CountryIso2')\n",
    "\n",
    "list_Daniele = \n",
    "\n",
    "temp = pd.read_csv(filepath_or_buffer = \"D:/Documents/2020/28_UNICEF/9_misc/compare_UNICEF_Transmonee.csv\", \n",
    "                      sep = ',',\n",
    "                      usecols = ['CountryDesc', 'CountryIso2', 'CountryIso3'])\n",
    "list_Daniele.columns = ['CountryDesc', 'CountryIso2', 'CountryIso3']\n",
    "\n",
    "list_Daniele = list_Daniele.append(temp)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the master country-reference lit\n",
    "'''\n",
    "url = 'https://pkgstore.datahub.io/core/country-list/data_csv/data/d7c9d7cfb42cb69f4422dec222dbbaa8/data_csv.csv'\n",
    "\n",
    "country_list = pd.read_csv(\n",
    "    filepath_or_buffer = url, \n",
    "    sep = ',',\n",
    "    )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducing Maplecroft transformation (dev section)\n",
    "\n",
    "Using indicator 3.1.1. as a first example (quantitative variable): Child labour rate (5-17) \n",
    "\n",
    "The maplecroft sheet has each indicator undergo a set of tranformations: \n",
    "\n",
    "1. Indicator raw data = basis\n",
    "2. Normalize indicator raw data with certain transformation (e.g. 0.3.1.1. becomes 0.3.1.1_s)\n",
    "3. Aggregate various normalized indicator data into a cluster value (e.g. 0.3.1.1 and 0.3.1.2. become 0.3_s)\n",
    "\n",
    "In the following I am reproducing these steps, each of which contains many different nested if-then statements\n",
    "\n",
    "## 1 Indicator raw data\n",
    "\n",
    "s. above\n",
    "\n",
    "## 2 Normalize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = min(s24_transformed.value.astype('float'))\n",
    "max_val = max(s24_transformed.value.astype('float'))\n",
    "tot_range = max_val - min_val \n",
    "\n",
    "if indicator = \n",
    "\n",
    "# Do the actual normalization\n",
    "s24_transformed.assign(norm_val = (s24_transformed.value.astype('float') - min_val)/ tot_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "These if-statements must be included: \n",
    "\n",
    "* If categorical --> Something I still have to figure out\n",
    "    * Else: If upper limit required --> take upper limit value.\n",
    "        * Else: Take max value of series. And then: \n",
    "            * If raw data value > max to use\n",
    "            \n",
    "* Categorical vs numerical\n",
    "    * Inverted\n",
    "        * Is raw data point (for a country) available? --> \n",
    "            * Max to use as upper limit or max value\n",
    "                * Min to use as lower limit of min value\n",
    "                    *  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "sklearn.preprocessing.MinMaxScaler().fit_transform(s24_transformed.value)\n",
    "\n",
    "\n",
    "\n",
    "# Create x, where x the 'scores' column's values as floats\n",
    "x = s24_transformed.value.astype(float)\n",
    "\n",
    "temp = x.reshape(1, -1)\n",
    "\n",
    "# Create a minimum and maximum processor object\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "# Create an object to transform the data to fit minmax processor\n",
    "x_scaled = min_max_scaler.fit_transform(temp)\n",
    "\n",
    "# Run the normalizer on the dataframe\n",
    "df_normalized = pd.DataFrame(x_scaled)\n",
    "\n",
    "\n",
    "# s56_transformed.head()\n",
    "\n",
    "# The maplecroft sheet has each indicator undergo the following tran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant columns\n",
    "temp = s24_raw[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "\n",
    "# Extract the latest value for each country\n",
    "temp[temp['timePeriodStart'] == temp.groupby('geoAreaName')['timePeriodStart'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the raw data\n",
    "s61_transformed = transform_sdg_api_data(raw_data = s61_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data and directly flatten it out into a pandas dataframe\n",
    "s24_raw = extract_sdg_api_data(series = 'VC_HTF_DETV')\n",
    "\n",
    "# Save data to raw data folder\n",
    "save_raw_data(dataframe = s24_raw,\n",
    "             filename = 'S_24.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data and directly flatten it out into a pandas dataframe\n",
    "url = 'https://unstats.un.org/SDGAPI/v1/sdg/Series/Data?seriesCode=VC_HTF_DETV&pageSize=999999999' # choose page size large enough to make sure data all fits in one page\n",
    "s24_raw = pd.json_normalize(requests.get(url).json()['data'])\n",
    "\n",
    "# Save data to raw data folder\n",
    "save_raw_data(dataframe = s24_raw,\n",
    "             filename = 'S_24.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract raw data\n",
    "s56_raw = s56_extract(subs_key = '460ab272abdd43c892bb59c218c22c09',\n",
    "                      start_period = '2010',\n",
    "                      end_period = '2020')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data(dataframe = s56_raw,\n",
    "             filename = 'S_56.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s56_extract(subs_key, start_period = 2010, end_period = 2020):\n",
    "    \"\"\"Extract raw data of source S-15 from the UNESCO api UIS\n",
    "\n",
    "    Parameters:\n",
    "    subs_key (string): Your API key. You must create one. Visit https://apiportal.uis.unesco.org/getting-started for more info\n",
    "    start_period (int): Year which is the beginning of series for which you want to collect data\n",
    "    end_period (ing): Year which is the end of series for which you want to collect data\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe and saves raw data to specified output_path\n",
    "\n",
    "   \"\"\"\n",
    "    url = 'https://api.uis.unesco.org/sdmx/data/SDG4/ROFST.PT.L3._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(syear = start_period,\n",
    "        eyear = end_period,\n",
    "        skey = subs_key)\n",
    "    return(pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data\n",
    "s55_raw = s55_extract(subs_key = '460ab272abdd43c892bb59c218c22c09',\n",
    "                      start_period = '2010',\n",
    "                      end_period = '2020')\n",
    "\n",
    "# Save raw data\n",
    "save_raw_data(dataframe = s55_raw,\n",
    "             filename = 'S_55.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s55_extract(subs_key, start_period = 2010, end_period = 2020):\n",
    "    \"\"\"Extract raw data from the UNESCO API\n",
    "\n",
    "    The API can be called with a API key, which you must generate by creating an account. \n",
    "    The calls can be done with simple https requests, in which many parameter must be specified (e.g. indicator code, sexes, age, ...). TO understand the structure, please check the query builder https://apiportal.uis.unesco.org/query-builder\n",
    "\n",
    "    Parameters:\n",
    "    subs_key (string): Your API key. You must create one. Visit https://apiportal.uis.unesco.org/getting-started for more info\n",
    "    start_period (int): Year which is the beginning of series for which you want to collect data\n",
    "    end_period (ing): Year which is the end of series for which you want to collect data\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe and saves raw data to specified output_path\n",
    "\n",
    "   \"\"\"\n",
    "    url = 'https://api.uis.unesco.org/sdmx/data/SDG4/ROFST.PT.L2._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(syear = start_period,\n",
    "        eyear = end_period,\n",
    "        skey = subs_key)\n",
    "    return(pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',))\n",
    "\n",
    "https://api.uis.unesco.org/sdmx/dataflow/all/all/latest?subscription-key=<your api key>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the relevant columns\n",
    "temp = s61_raw[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "\n",
    "# Extract the latest value for each country\n",
    "temp = temp[temp['timePeriodStart'] == temp.groupby('geoAreaName')['timePeriodStart'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract the relevant columns\n",
    "temp = temp[['geoAreaCode',\n",
    "            'geoAreaName',\n",
    "            'timePeriodStart',\n",
    "            'value']]\n",
    "\n",
    "# Extract the latest value for each country\n",
    "temp = temp[temp['timePeriodStart'] == temp.groupby('geoAreaName')['timePeriodStart'].transform('max')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming ILO data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows to get both sexes\n",
    "temp  = s23_raw_ilo[s23_raw_ilo['SEX'] == 'SEX_T']\n",
    "\n",
    "# Extract the relevant columns\n",
    "temp = temp[['REF_AREA',\n",
    "            'TIME_PERIOD',\n",
    "            'OBS_VALUE']]\n",
    "\n",
    "# Extract the latest value for each country\n",
    "temp = temp[temp['TIME_PERIOD'] == temp.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]\n",
    "\n",
    "# Save file to examine it\n",
    "temp.to_excel(cwd + \"\\data_transformed\\S_23_SDG_ILO.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting data through ILO API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from ILO API. The link is generated from this website: https://ilostat.ilo.org/data/sdmx-query-builder/ . \n",
    "# The code of the indicator I have from here: \n",
    "url = 'https://www.ilo.org/sdmx/rest/data/ILO,DF_SDG_ALL_SDG_A831_SEX_RT/?format=csv&startPeriod=2010-01-01&endPeriod=2020-12-31'\n",
    "\n",
    "s23_raw_ilo = pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',)\n",
    "\n",
    "# Save data to raw data folder\n",
    "s23_raw_ilo.to_excel(dataframe = s23_raw,\n",
    "             filename = 'S_23.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s56_save_raw(s56_data, output_path = data_sources_raw + '\\S_56.xlsx'):\n",
    "    \"\"\"Save raw data of source s56 as .xlsx file\n",
    "\n",
    "    Parameters:\n",
    "    s15_data (obj): Raw data to be be saved. Must be ob class pandas.DataFrame\n",
    "    output_path (string): Output path of where to save the raw data if save_raw is set to true\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe and saves raw data to specified output_path\n",
    "\n",
    "   \"\"\"\n",
    "    s56_data.to_excel(output_path)\n",
    "    print('The raw data has been saved as .xlsx file in: ' + output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def s55_save_raw(s55_data, output_path = data_sources_raw + '\\S_55.xlsx'):\n",
    "    \"\"\"Save raw data of source s55 as .xlsx file\n",
    "\n",
    "    Parameters:\n",
    "    s15_data (obj): Raw data to be be saved. Must be ob class pandas.DataFrame\n",
    "    output_path (string): Output path of where to save the raw data if save_raw is set to true\n",
    "\n",
    "    Returns:\n",
    "    obj: Returns pandas dataframe and saves raw data to specified output_path\n",
    "\n",
    "   \"\"\"\n",
    "    s55_data.to_excel(output_path)\n",
    "    print('The raw data has been saved as .xlsx file in: ' + output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the most up-to-date number for each country\n",
    "s16_data[s16_data['TIME_PERIOD']==s16_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard unnecessay columns\n",
    "s16_data = s16_data[['REF_AREA',\n",
    "         'TIME_PERIOD',\n",
    "         'OBS_VALUE']]\n",
    "\n",
    "# Discard unnecessary rows\n",
    "s16_data = s16_data[s16_data.REF_AREA.isin(counry_list.Code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters of the API query\n",
    "subs_key = '460ab272abdd43c892bb59c218c22c09' # must generate an KEY on the UIS page\n",
    "start_period = '2010' # define from which year you wan to retrieve data \n",
    "end_period = '2020' # define until what you want to retreive data\n",
    "\n",
    "# Build URL query based on parameters defined\n",
    "url = 'https://api.uis.unesco.org/sdmx/data/SDG4/ROFST.PT.L3._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(syear = start_period,\n",
    "eyear = end_period,\n",
    "skey = subs_key)\n",
    "\n",
    "# Extract the data\n",
    "s16_data = pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',\n",
    "                      )\n",
    "\n",
    "# Save data in raw version as excel file # TO do: Create a versioning\n",
    "s16_data.to_excel(data_sources_raw + '\\S_16.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the most up-to-date number for each country\n",
    "s15_data[s15_data['TIME_PERIOD']==s15_data.groupby('REF_AREA')['TIME_PERIOD'].transform('max')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard unnecessay columns\n",
    "s15_data = s15_data[['REF_AREA',\n",
    "         'TIME_PERIOD',\n",
    "         'OBS_VALUE']]\n",
    "\n",
    "# Discard unnecessary rows\n",
    "s15_data = s15_data[s15_data.REF_AREA.isin(counry_list.Code)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters of the API query\n",
    "subs_key = '460ab272abdd43c892bb59c218c22c09' # must generate an KEY on the UIS page\n",
    "start_period = '2010' # define from which year you wan to retrieve data \n",
    "end_period = '2020' # define until what you want to retreive data\n",
    "\n",
    "# Build URL query based on parameters defined\n",
    "url = 'https://api.uis.unesco.org/sdmx/data/SDG4/ROFST.PT.L2._T._T.SCH_AGE_GROUP._T.INST_T._Z._T._Z._Z._Z._T._T._Z._Z._Z.?startPeriod={syear}&endPeriod={eyear}&format=csv-sdmx&subscription-key={skey}'.format(syear = start_period,\n",
    "eyear = end_period,\n",
    "skey = subs_key)\n",
    "\n",
    "# Extract the data\n",
    "s15_data = pd.read_csv(filepath_or_buffer = url, \n",
    "                      sep = ',',\n",
    "                      )\n",
    "\n",
    "# Save data in raw version as excel file # TO do: Create a versioning\n",
    "s15_data.to_excel(data_sources_raw + '\\S_15.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s15_data.REF_AREA.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see if ILO data can be retrieved from their SDMX data warehouse\n",
    "url = \"https://www.ilo.org/sdmx/rest/data/ILO,DF_YI_ALL_IFL_4IEM_SEX_ECO_IFL_RT/?format=jsondata&startPeriod=2010-01-01&endPeriod=2020-12-31\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "my_json = json.loads(response.text)\n",
    "\n",
    "type(my_json)\n",
    "\n",
    "# response = json.loads(response.text)\n",
    "\n",
    "print(my_json)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check out the UN SDG Goals API\n",
    "\n",
    "url = \"https://unstats.un.org/SDGAPI/v1/sdg/Indicator/VC_HTF_DETV%20/Series/List\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "my_json = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S-11\n",
    "url = 'https://outoftheshadows.eiu.com/wp-content/uploads/2019/05/OOSI_Out_of_the_shadows_index_60-countries_May2019.xlsm'\n",
    "\n",
    "data = pd.read_excel(url,\n",
    "                    sheet_name = 'Dataset',\n",
    "                    skiprows = 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s15_data.groupby(['REF_AREA'], sort = False)[['TIME_PERIOD', 'OBS_VALUE']].max()\n",
    "# s15_data.groupby(['REF_AREA'], sort = False).max()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "274.633px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
